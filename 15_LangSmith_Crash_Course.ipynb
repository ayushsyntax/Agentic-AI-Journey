{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMaSf6F2JC2p9Mi94Gk2c+e"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **Why Observability Matters for LLM Production**\n",
        "\n",
        "Large Language Model (LLM) systems behave very differently from traditional code. They introduce **non-determinism**, **probabilistic outputs**, **multi-stage pipelines**, and **hidden costs** — requiring specialized tooling to detect, trace, and fix issues in production.\n",
        "\n",
        "![Image](https://mintcdn.com/langchain-5e9cc07a/H9jA2WRyA-MV4-H0/langsmith/images/project.png?auto=format\\&fit=max\\&n=H9jA2WRyA-MV4-H0\\&q=85\\&s=2426200ab2e619674636e41f11246c0d)\n",
        "\n",
        "![Image](https://raw.githubusercontent.com/langchain-ai/langsmith-cookbook/1cc7d013abfabdd3e92c3f7cfc04498669e74a8b/tracing-examples/traceable/img/snapshot_1.png)\n",
        "\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2ADWA5A6o0RxRqdtAEyJwryA.png)\n",
        "\n",
        "According to official documentation, LangSmith provides *end-to-end observability* — tracing, logging, dashboards, alerting — tailored for complex LLM workflows like chains, agents, and RAG pipelines. ([LangChain Docs][1])\n",
        "Below is the **mapping** between core production problems and how observability solves them:\n",
        "\n",
        "### **1) Latency Spikes in Complex Workflows**\n",
        "\n",
        "**Problem:** Multi-stage workflows (e.g., document processing → retrieval → LLM reasoner → summarizer) can have unexpected spikes in runtime. Without component-level visibility, you cannot isolate the bottleneck.\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Granular Tracing:** A trace represents an entire request execution; individual **runs** represent steps.\n",
        "* **Detailed Timings:** Each run logs start/end time, allowing pinpointing slow stages.\n",
        "\n",
        "**Mapped To:** Component-level tracing and timing in LangSmith. ([LangChain Docs][1])\n",
        "\n",
        "### **2) Uncontrolled Cost Spikes**\n",
        "\n",
        "**Problem:** Minor prompt changes or misbehaving agent loops can dramatically increase token usage and costs.\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Token Usage Metrics:** Traces include counts of input and output tokens.\n",
        "* **Cost Attribution:** Automatic cost calculation per run based on model pricing.\n",
        "\n",
        "**Mapped To:** Token logging + cost insights. ([LangChain Docs][1])\n",
        "\n",
        "### **3) RAG Hallucinations**\n",
        "\n",
        "**Problem:** When a RAG system hallucinates, you need to know: was the retriever broken, or did the LLM misinterpret the retrieved facts?\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Mid-Step Inspection:** Retriever results, retriever queries, and final prompt contents are logged.\n",
        "* **Sequenced Trace:** You can inspect each run in a trace, including RAG sub-steps.\n",
        "\n",
        "**Mapped To:** Intermediate step inspection in LangSmith traces. ([LangChain Docs][2])\n",
        "\n",
        "### **4) Non-Deterministic Behavior**\n",
        "\n",
        "**Problem:** LLMs can produce different outputs for the same input, confounding traditional debugging.\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Full Run Logs:** Every run captures context, inputs, and outputs.\n",
        "* **Comparison Over Time:** Teams can compare deviations statistically or with monitoring.\n",
        "\n",
        "**Mapped To:** Traces record every run state and shared visibility across teams. ([LangChain Docs][1])\n",
        "\n",
        "### **5) Graph Execution Complexity**\n",
        "\n",
        "**Problem:** Orchestrators like LangGraph with parallel/branching paths make it hard to know which node failed.\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Node-to-Run Mapping:** Each graph node corresponds to a run with full context.\n",
        "* **Detailed Failure Info:** Stack traces and output logs help isolate faulty graph paths.\n",
        "\n",
        "**Mapped To:** Visualization of flows via trace trees. ([Analytics Vidhya][3])\n",
        "\n",
        "### **6) Inefficient Data Processing**\n",
        "\n",
        "**Problem:** Repeated expensive preprocessing (like chunking PDFs) delays every request and consumes compute wastefully.\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Persistent Workflow Logging:** Detect redundant calls across traces and optimize with caching or indexing.\n",
        "\n",
        "**Mapped To:** Logging and persistent intermediate results in LangSmith. ([LangChain Docs][1])\n",
        "\n",
        "### **7) Partial Tracing Gaps**\n",
        "\n",
        "**Problem:** Many tools trace only LLM calls and miss custom Python logic (e.g., splitting, embedding, I/O).\n",
        "\n",
        "**Why Observability Helps:**\n",
        "\n",
        "* **Manual Instrumentation:** Decorators / wrappers can trace custom functions.\n",
        "* **Unified View:** Full trace trees include remote calls and local Python logic.\n",
        "\n",
        "**Mapped To:** `@traceable` instrumentation in LangSmith. ([LangChain Docs][4])\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Concepts in LangSmith**\n",
        "\n",
        "LangSmith organizes execution metadata hierarchically:\n",
        "\n",
        "* **Project** – Logical container for all observed workflows.\n",
        "* **Trace** – A single end-to-end execution (e.g., one user request).\n",
        "* **Run** – A discrete step within a trace (prompt, tool call, custom Python step). ([LangChain Docs][1])\n",
        "\n",
        "---\n",
        "\n",
        "## **Production-Ready Setup & Integration**\n",
        "\n",
        "Below is a **self-contained, structured implementation** that you can adopt in any LangChain or custom Python LLM app.\n",
        "\n",
        "### **Environment Configuration (shell / .env)**\n",
        "\n",
        "```bash\n",
        "# Required for LangSmith tracing + project identification\n",
        "export LANGCHAIN_TRACING_V2=\"true\"\n",
        "export LANGCHAIN_ENDPOINT=\"https://api.smith.langchain.com\"\n",
        "export LANGCHAIN_API_KEY=\"<YOUR_LANGSMITH_API_KEY>\"\n",
        "export LANGCHAIN_PROJECT=\"my_production_project\"\n",
        "\n",
        "# Also set your LLM API key (e.g., OpenAI)\n",
        "export OPENAI_API_KEY=\"<YOUR_OPENAI_KEY>\"\n",
        "```\n",
        "\n",
        "These environment variables enable automatic tracing and tagging with a consistent project. ([LangChain Docs][5])\n",
        "\n",
        "---\n",
        "\n",
        "## **Minimal Observability Example (LangChain)**\n",
        "\n",
        "This example runs a simple LLM chain and traces all steps.\n",
        "\n",
        "```python\n",
        "import os\n",
        "from langchain import LLMChain, PromptTemplate\n",
        "from langchain.llms import OpenAI\n",
        "\n",
        "# Must be set in environment beforehand\n",
        "# Already enabled with LANGCHAIN_TRACING_V2=true\n",
        "\n",
        "llm = OpenAI(model=\"gpt-4o\", temperature=0.7)\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"query\"],\n",
        "    template=\"Summarize this: {query}\"\n",
        ")\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "\n",
        "# config allows tagging and metadata\n",
        "config = {\n",
        "    \"tags\": [\"summary-task\", \"prod\"],\n",
        "    \"metadata\": {\"model\": \"gpt-4o\", \"version\": \"1.0\"}\n",
        "}\n",
        "\n",
        "result = chain.invoke({\"query\": \"Explain LangSmith observability in your own words.\"}, config=config)\n",
        "print(result.output)\n",
        "```\n",
        "\n",
        "**Notes:**\n",
        "\n",
        "* Each `.invoke()` automatically creates a trace with nested runs for prompt → model → parsing.\n",
        "* Tags and metadata help search and filter in the UI. ([LangChain Docs][1])\n",
        "\n",
        "---\n",
        "\n",
        "## **Tracing Custom Python Logic**\n",
        "\n",
        "When your workflow includes non-LangChain Python steps (PDF loaders, custom retrievers), annotate them:\n",
        "\n",
        "```python\n",
        "from langsmith import traceable\n",
        "\n",
        "@traceable(name=\"Load_PDF\")\n",
        "def load_pdf(path: str) -> str:\n",
        "    # Actual PDF loading logic\n",
        "    with open(path, \"rb\") as f:\n",
        "        raw = f.read()\n",
        "    return raw\n",
        "\n",
        "@traceable(name=\"Chunk_Text\")\n",
        "def split_text(text: str) -> list[str]:\n",
        "    # Chunking logic (for RAG pipelines, etc.)\n",
        "    chunks = text.split(\"\\n\\n\")\n",
        "    return chunks\n",
        "```\n",
        "\n",
        "This ensures these steps appear as runs inside traces alongside LLM calls. ([LangChain Docs][4])\n",
        "\n",
        "---\n",
        "\n",
        "## **Building Evaluation Datasets**\n",
        "\n",
        "To proactively test for correctness and avoid RAG hallucinations or regressions:\n",
        "\n",
        "1. Create a dataset in the LangSmith UI.\n",
        "2. Use trace runs to add examples (including “reference outputs”).\n",
        "3. Run automated evaluations against new prompt versions. ([LangChain Docs][6])\n",
        "\n",
        "Datasets can be imported from CSV/JSONL or built directly from observed traces. ([LangChain Docs][6])\n",
        "\n",
        "---\n",
        "\n",
        "## **Advanced Monitoring & Dashboards**\n",
        "\n",
        "LangSmith offers dashboards that show:\n",
        "\n",
        "* Total traces over time\n",
        "* Error rates\n",
        "* Average latency per run\n",
        "* Token usage & cost trends\n",
        "* Alert triggers (e.g., slow runs, token cost drift) ([LangChain Docs][7])\n",
        "\n",
        "These help you monitor the health of your LLM pipeline at scale.\n",
        "\n",
        "---\n",
        "\n",
        "## **Quick Summary (Production Challenges ↔ Solutions)**\n",
        "\n",
        "| Problem               | Observability Solution              |\n",
        "| --------------------- | ----------------------------------- |\n",
        "| Latency spikes        | Timed runs & bottleneck detection   |\n",
        "| Cost spikes           | Token + cost attribution            |\n",
        "| RAG hallucinations    | Inspect retriever vs. LLM stages    |\n",
        "| Non-determinism       | Full trace logs for reproducibility |\n",
        "| Graph flow complexity | Node → Run mapping                  |\n",
        "| Redundant processing  | Persistent trace insights           |\n",
        "| Partial visibility    | Decorator instrumentation           |\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Jx7hkz-wO4Zu"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **Q&A: Mental Models for Mastering LangSmith**\n",
        "\n",
        "**Q1. Why do LLM applications need observability at all?**\n",
        "Because LLM systems are probabilistic, multi-step, cost-bearing, non-deterministic pipelines. Traditional logs tell you what happened; observability tells you why. That distinction matters when a workflow jumps from 2 → 10 minutes or ₹0.50 → ₹2.00 per run with no code change.\n",
        "\n",
        "---\n",
        "\n",
        "**Q2. What’s the difference between a Trace and a Run?**\n",
        "A **Trace** is the entire “user request → final output” execution.\n",
        "A **Run** is a single step inside a trace (e.g., Retriever, Prompt, LLM, Parser, Custom Function).\n",
        "\n",
        "Traces show *end-to-end behavior*; runs show the *component anatomy* of that behavior.\n",
        "\n",
        "---\n",
        "\n",
        "**Q3. Why do Runs matter in debugging RAG hallucinations?**\n",
        "Because hallucinations come in two flavors:\n",
        "• retriever failure (bad docs)\n",
        "• generator failure (bad reasoning)\n",
        "\n",
        "Runs let you inspect intermediate artifacts: query vectors, retrieved chunks, combined prompt, and final output. Without that, you’re stuck shrugging at the ceiling.\n",
        "\n",
        "---\n",
        "\n",
        "**Q4. Why does LangSmith help with cost and token explosions?**\n",
        "Because it logs input/output token counts per run, aggregates across traces, and ties it to model pricing. The “perfectionist loop” pathology in agents becomes visible instead of silently incinerating money.\n",
        "\n",
        "---\n",
        "\n",
        "**Q5. Why does LangGraph integrate nicely with LangSmith?**\n",
        "Graph nodes map cleanly to runs, which means branches, conditionals, loops, and parallelism become inspectable trees instead of inscrutable spaghetti.\n",
        "\n",
        "---\n",
        "\n",
        "**Q6. Why are custom Python functions traceable?**\n",
        "Because LLM apps aren’t just prompts—they’re I/O, parsing, chunking, embedding, indexing, retrieval, caching, vector stores, tool calling, etc. Decorated custom functions close the gap so you get a full white-box view instead of a half-lit cave.\n",
        "\n",
        "---\n",
        "\n",
        "**Q7. Why do we evaluate before deploying prompt/model changes?**\n",
        "Because without evaluation, every change is a dice roll. With evaluation sets, you get regression testing for a probabilistic system—arguably more important than in deterministic software due to non-repeatability.\n",
        "\n",
        "---\n",
        "\n",
        "**Q8. How do monitoring and alerting differ from observability?**\n",
        "Observability answers: “What happened in this one execution?”\n",
        "Monitoring answers: “What are trends across 10,000 executions?”\n",
        "Alerting answers: “Should a human be waking up right now?”\n",
        "\n",
        "---\n",
        "\n",
        "# **Key Points to Remember for Mastery**\n",
        "\n",
        "These are the distilled ideas that turn you from “user” to “operator”:\n",
        "\n",
        "**1. LLM pipelines are graphs, not function calls.**\n",
        "Nodes have latency, cost, error modes, and nondeterministic behaviors.\n",
        "\n",
        "**2. Traces and Runs give you causal visibility.**\n",
        "Enough to attribute hallucinations, latency spikes, and cost explosions.\n",
        "\n",
        "**3. RAG systems require intermediate inspection.**\n",
        "Retriever errors vs. generator errors are structurally different failure modes.\n",
        "\n",
        "**4. Non-determinism is normal, not a bug.**\n",
        "Debugging requires recorded execution state, not reliance on reproduction.\n",
        "\n",
        "**5. Token accounting matters in production.**\n",
        "Every prompt is a billing artifact and must be measured as such.\n",
        "\n",
        "**6. Observability ≠ Monitoring ≠ Evaluation.**\n",
        "Each solves a different part of the LLM lifecycle puzzle.\n",
        "\n",
        "---\n",
        "\n",
        "# **If you want a fast mnemonic**\n",
        "\n",
        "Think of **LLM Production ≈ three verbs:**\n",
        "\n",
        "> **See → Compare → Predict**\n",
        "\n",
        "LangSmith maps cleanly to that:\n",
        "\n",
        "| Verb        | Feature                       |\n",
        "| ----------- | ----------------------------- |\n",
        "| **See**     | Traces + Runs (observability) |\n",
        "| **Compare** | Evaluations + Datasets        |\n",
        "| **Predict** | Monitoring + Alerting         |\n",
        "\n",
        "Once that clicks, the tool’s architecture stops feeling like a magical debugging shrine and starts feeling like normal engineering.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "tFZPwHKZPeel"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kScHXomzPmDG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}