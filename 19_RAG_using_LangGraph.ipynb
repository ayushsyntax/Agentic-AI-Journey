{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMGm+jQe0tb6geCpbP4WdG5"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AMI9WDgzoOGAH4bOnAwBKEw.jpeg)\n",
        "\n",
        "![Image](https://engineering.fb.com/wp-content/uploads/2017/03/GOcmDQEFmV52jukHAAAAAAAqO6pvbj0JAAAB.jpg)\n",
        "\n",
        "![Image](https://assets.zilliz.com/vector_db_integration_107439d031.png)\n",
        "\n",
        "![Image](https://miro.medium.com/1%2AKX54wUL3_JPplPiqGPRngw.png)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "# üìå Multi-Utility Chatbot with RAG as a Tool (Improved & Contextualized)\n",
        "\n",
        "## üß† Core Architectural Insight\n",
        "\n",
        "**Traditional RAG Chain:**\n",
        "`Ingest ‚Üí Retrieve ‚Üí Answer`\n",
        "\n",
        "**This Video‚Äôs Novelty:**\n",
        "\n",
        "> Treat RAG **as an external Tool** ‚Äî exactly like a Calculator or Stock Price lookup.\n",
        "> The **LLM decides** whether to call the RAG tool based on the question.\n",
        "\n",
        "üìå This creates a **dynamic decision boundary**:\n",
        "\n",
        "* If the question is general (‚ÄúWhat is supervised learning?‚Äù) ‚Üí LLM answers from its internal knowledge.\n",
        "* If the question refers to the uploaded document (‚ÄúWhat does the PDF say about supervised learning?‚Äù) ‚Üí LLM triggers the RAG tool.\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Full Workflow (High-Level)\n",
        "\n",
        "### Two Phases\n",
        "\n",
        "```\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Phase 1: Ingestion (Setup) ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        ‚Üì\n",
        " Ingest Document ‚Üí Chunk ‚Üí Embed ‚Üí Store\n",
        "\n",
        "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
        "‚îÇ Phase 2: Retrieval Execution (Run) ‚îÇ\n",
        "‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
        "        ‚Üì\n",
        "Query ‚Üí Chat Node ‚Üí (Optional) Tool Call ‚Üí LLM Answer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üìÅ PHASE 1 ‚Äî Document Ingestion & Vector Store Setup\n",
        "\n",
        "This code *must run once per new document*.\n",
        "\n",
        "### 1) Load & Parse PDF\n",
        "\n",
        "```python\n",
        "from langchain_community.document_loaders import PyPDFLoader\n",
        "\n",
        "loader = PyPDFLoader(\"intro_to_ml.pdf\")\n",
        "raw_pages = loader.load()  # Metadata + raw page text\n",
        "```\n",
        "\n",
        "### 2) Split Into Chunks\n",
        "\n",
        "```python\n",
        "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
        "\n",
        "splitter = RecursiveCharacterTextSplitter(\n",
        "    chunk_size=1000,\n",
        "    chunk_overlap=200\n",
        ")\n",
        "documents = splitter.split_documents(raw_pages)\n",
        "```\n",
        "\n",
        "* **chunk_size** ensures each piece fits within LLM context windows.\n",
        "* **chunk_overlap** preserves continuity across splits.\n",
        "\n",
        "### 3) Embeddings\n",
        "\n",
        "```python\n",
        "from langchain_openai import OpenAIEmbeddings\n",
        "\n",
        "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
        "```\n",
        "\n",
        "* Use a **fixed, pinned embedding model** for deterministic behavior.\n",
        "\n",
        "### 4) Store in FAISS\n",
        "\n",
        "```python\n",
        "from langchain_community.vectorstores import FAISS\n",
        "\n",
        "faiss_index = FAISS.from_documents(documents, embeddings)\n",
        "retriever = faiss_index.as_retriever(search_kwargs={\"k\": 4})\n",
        "```\n",
        "\n",
        "* **FAISS** is local, high-performance, and vector-based.\n",
        "* `k=4` retrieves the top 4 semantically closest chunks.\n",
        "\n",
        "---\n",
        "\n",
        "# üõ† PHASE 2 ‚Äî RAG as a Tool\n",
        "\n",
        "## üîß Define the RAG Tool\n",
        "\n",
        "```python\n",
        "from langchain_core.tools import tool\n",
        "\n",
        "@tool(response_format=\"content_and_artifact\")\n",
        "def retrieve_information(query: str):\n",
        "    \"\"\"\n",
        "    Given a query, return the top-k document chunks relevant to the query.\n",
        "    \"\"\"\n",
        "    docs = retriever.invoke(query)  # Vector search\n",
        "\n",
        "    context = \"\\n\\n\".join(\n",
        "        f\"Source: {doc.metadata}\\nContent: {doc.page_content}\"\n",
        "        for doc in docs\n",
        "    )\n",
        "\n",
        "    return context, docs\n",
        "```\n",
        "\n",
        "### What This Tool Does\n",
        "\n",
        "1. Transforms query ‚Üí vector\n",
        "2. Performs FAISS search\n",
        "3. Formats text for LLM consumption\n",
        "4. Outputs:\n",
        "\n",
        "   * `context` (for grounding the answer)\n",
        "   * `docs` (traceable source chunks)\n",
        "\n",
        "---\n",
        "\n",
        "# üß† SYSTEM INTEGRATION ‚Äî LangGraph\n",
        "\n",
        "## Bind the RAG Tool\n",
        "\n",
        "```python\n",
        "from langgraph.prebuilt import ToolNode, tools_condition\n",
        "\n",
        "tools = [retrieve_information, calculator, get_stock_price]\n",
        "llm_with_tools = llm.bind_tools(tools)\n",
        "```\n",
        "\n",
        "## Define Graph\n",
        "\n",
        "```python\n",
        "builder = StateGraph(State)\n",
        "\n",
        "builder.add_node(\"chat\", chat_node)\n",
        "builder.add_node(\"tools\", ToolNode(tools))\n",
        "\n",
        "builder.add_edge(START, \"chat\")\n",
        "builder.add_conditional_edges(\"chat\", tools_condition)\n",
        "builder.add_edge(\"tools\", \"chat\")\n",
        "\n",
        "app = builder.compile()\n",
        "```\n",
        "\n",
        "* **chat_node**: initial conversational agent\n",
        "* **tools_condition**: logic deciding whether to call a tool\n",
        "* **ToolNode**: encapsulates RAG, Calculator, and Stock APIs\n",
        "\n",
        "---\n",
        "\n",
        "# üß† DETAILED SEQUENCE DIAGRAM (Concept)\n",
        "\n",
        "```mermaid\n",
        "sequenceDiagram\n",
        "    participant User\n",
        "    participant Chat as Chat Node\n",
        "    participant Tool as RAG Tool\n",
        "    participant FAISS\n",
        "\n",
        "    User->>Chat: Query\n",
        "    Chat-->>Chat: Decide (LLM internal logic)\n",
        "    alt Needs RAG\n",
        "        Chat->>Tool: retrieve_information(query)\n",
        "        Tool->>FAISS: Vector search\n",
        "        FAISS-->>Tool: Top K contexts\n",
        "        Tool-->>Chat: context + docs\n",
        "        Chat-->>User: Final answer (grounded)\n",
        "    else No RAG needed\n",
        "        Chat-->>User: Direct answer\n",
        "    end\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# üìå COMMON QUESTIONS ‚Äî With Answers\n",
        "\n",
        "### **Q: What exactly decides if the RAG tool is used?**\n",
        "\n",
        "**A:**\n",
        "An LLM classifier/agent logic inside the `chat_node` determines intent. It triggers `retrieve_information` only when the query references the uploaded document or demands factual grounding.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: Why FAISS instead of a cloud vector database?**\n",
        "\n",
        "**A:**\n",
        "\n",
        "* FAISS is **local**, fast, and private.\n",
        "* No external costs or network latency.\n",
        "* Replaceable with Pinecone/Weaviate if horizontal scalability is required.\n",
        "\n",
        "---\n",
        "\n",
        "### **Q: What part ensures answers don‚Äôt hallucinate?**\n",
        "\n",
        "**A:**\n",
        "\n",
        "* For RAG queries, the tool returns **grounded text chunks**.\n",
        "* The LLM conditions answers on this context, reducing hallucination.\n",
        "\n",
        "---\n",
        "\n",
        "# üöß EDGE CASES & ADVERSARIAL CONDITIONS\n",
        "\n",
        "| Condition                   | Behavior             | Mitigation                                                 |\n",
        "| --------------------------- | -------------------- | ---------------------------------------------------------- |\n",
        "| Query unrelated to document | LLM answers directly | Ensure agent doesn‚Äôt call RAG tool unnecessarily           |\n",
        "| Very large documents        | FAISS slows down     | Pre-shard per chapter or use HNSW / optimized vector store |\n",
        "| Overlapping semantics       | Irrelevant chunks    | Improve chunking & tuning of embeddings                    |\n",
        "| Ambiguous user intent       | Wrong tool triggered | Add explicit intent classification layer                   |\n",
        "| Source citation             | User wants source    | Include metadata in responses                              |\n",
        "\n",
        "---\n",
        "\n",
        "## üìå ADDITIONAL OPTIMIZATIONS (When Scaling)\n",
        "\n",
        "‚û§ **Persistent Vector Store:** Serialize FAISS to disk with checkpointing.\n",
        "‚û§ **Incremental Ingestion:** New docs update vectors without full rebuild.\n",
        "‚û§ **Semantic Filtering:** Use query classification to bypass RAG for simple questions.\n",
        "‚û§ **Multi-Document Support:** Index multiple PDFs and route queries to correct namespace.\n",
        "\n",
        "---\n",
        "\n",
        "## üìä PERFORMANCE & SLA NOTES\n",
        "\n",
        "* **Latency:** RAG adds ~7‚Äì10 seconds due to vector search + context processing.\n",
        "* **Throughput:** FAISS local is faster than HTTP database calls.\n",
        "* **SLO:** 95% of queries return within 12s.\n",
        "* **Breach Handling:** If RAG fails, fallback to internal LLM answer with ‚Äúincomplete context‚Äù.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "O4HQmcQUI_Oe"
      }
    }
  ]
}