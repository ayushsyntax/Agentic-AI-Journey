{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN3Z1CVxq0IACGkL3hvWh2p"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# **First Principles of Memory — Minimal Summary**\n",
        "\n",
        "---\n",
        "\n",
        "## **1. Physics of Statelessness**\n",
        "\n",
        "LLMs are defined by:\n",
        "\n",
        "[\n",
        "y = f(x, \\theta)\n",
        "]\n",
        "\n",
        "* **θ (Theta):** fixed model parameters (**Parametric Knowledge**)\n",
        "* **x (Input):** user-controlled prompt\n",
        "\n",
        "Since θ does not update during inference and (y) depends solely on current (x), LLMs are **Stateless**.\n",
        "Memory is **simulated** via **In-Context Learning** (injecting history into (x)).\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Memory Architecture**\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize\\:fit:1400/1*W55mlf_j7MqqLwhkREqbjg.jpeg)\n",
        "\n",
        "### **A. Short-Term Memory (STM)**\n",
        "\n",
        "* **Thread-Scoped**\n",
        "* Implemented as a **Conversation Buffer**\n",
        "* **Stranger Problem:** new threads lose continuity → no personalization\n",
        "\n",
        "### **B. Long-Term Memory (LTM)**\n",
        "\n",
        "* **Cross-Thread**\n",
        "* Persists beyond a single session\n",
        "* Three categories:\n",
        "\n",
        "  1. **Episodic** (past events)\n",
        "  2. **Semantic** (facts)\n",
        "  3. **Procedural** (strategies)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. LTM Engineering Pipeline**\n",
        "\n",
        "To support cross-thread reasoning, LTM runs alongside the chat:\n",
        "\n",
        "1. **Creation:** extract Memory Candidates\n",
        "2. **Storage:** persist to Vector/SQL with metadata\n",
        "3. **Retrieval:** **Selective Search** on current query\n",
        "4. **Injection:** feed retrieved items into Context Window\n",
        "\n",
        "---\n",
        "\n",
        "## **4. Managed Memory Layers**\n",
        "\n",
        "Because manual extraction/retrieval is difficult:\n",
        "\n",
        "* Suggested libraries: **LangMem**, **Mem0**, **SuperMemory**\n",
        "* Research direction: Transformers with **Intrinsic Memory** (Google Titans + Mirage)\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "TSEldYu3xia5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## **0. Context**\n",
        "\n",
        "This deep dive focuses on the **theoretical architecture** required to build agents capable of memory. Unlike coding tutorials, this video explains **how memory must be engineered** around stateless LLMs to enable advanced behaviors.\n",
        "\n",
        "---\n",
        "\n",
        "## **1. The Physics of LLM Memory**\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize\\:fit:1400/1*06XFCeTdjIIhnE0dBX-OmA.png)\n",
        "\n",
        "**The Math:**\n",
        "\n",
        "[\n",
        "y = f(x, \\theta)\n",
        "]\n",
        "\n",
        "* **θ (Theta):** Model weights → fixed after training (**Parametric Knowledge**)\n",
        "* **x (Input):** User-controlled variable\n",
        "\n",
        "**Key Constraint:**\n",
        "\n",
        "* LLMs are **Stateless Mathematical Functions**\n",
        "* After computing (y_1 = f(x_1, θ)), the model **forgets** (x_1)\n",
        "* Therefore, LLMs have **no intrinsic memory**\n",
        "\n",
        "**Resulting Paradox:**\n",
        "\n",
        "* Memory must be engineered **externally** by manipulating (x)\n",
        "\n",
        "**Practical Hack (The Trick):**\n",
        "\n",
        "* Use the **Context Window** + **In-Context Learning**\n",
        "* Replay past tokens into new prompts to simulate memory\n",
        "\n",
        "---\n",
        "\n",
        "## **2. Memory Architectures**\n",
        "\n",
        "The video introduces two architectures based on **scope** and **lifespan**:\n",
        "\n",
        "### **A. Short-Term Memory (STM)**\n",
        "\n",
        "![Image](https://substackcdn.com/image/fetch/f_auto,q_auto\\:good,fl_progressive\\:steep/https://substack-post-media.s3.amazonaws.com/public/images/97e715a6-b270-4ed6-813a-17111e4203d2_1200x1200.png)\n",
        "\n",
        "**Properties:**\n",
        "\n",
        "* **Thread-Scoped** (single session)\n",
        "* Implemented as a **Conversation Buffer**\n",
        "* Mechanism: replay history into the context window each turn\n",
        "\n",
        "**Key Problems:**\n",
        "\n",
        "1. **Fragility:** RAM-based → wiped on restart\n",
        "2. **Overflow:** exceeding token limits causes crashes/hallucination\n",
        "\n",
        "   * requires **Trimming** or **Summarization**\n",
        "3. **Isolation:** cannot recall data across threads (no “yesterday memory”)\n",
        "\n",
        "---\n",
        "\n",
        "### **B. Long-Term Memory (LTM)**\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize\\:fit:1400/1*W55mlf_j7MqqLwhkREqbjg.jpeg)\n",
        "\n",
        "**Goal:** Store information that survives beyond a single conversation\n",
        "\n",
        "**Three Types:**\n",
        "\n",
        "1. **Episodic:** past events\n",
        "   *example: “User rejected the last draft”*\n",
        "2. **Semantic:** facts\n",
        "   *example: “User is a Python developer”*\n",
        "3. **Procedural:** strategies\n",
        "   *example: “Always use step-by-step reasoning for this user”*\n",
        "\n",
        "**Scope:** **Cross-Thread** (supports multi-session continuity)\n",
        "\n",
        "---\n",
        "\n",
        "## **3. Implementation Pipelines**\n",
        "\n",
        "### **A. Implementing STM**\n",
        "\n",
        "**Method:**\n",
        "\n",
        "* Maintain a `messages` list\n",
        "* Refeed history each turn:\n",
        "\n",
        "[\n",
        "y_2 = f(x_1 + y_1 + x_2)\n",
        "]\n",
        "\n",
        "**Optimizations:**\n",
        "\n",
        "* Keep last N messages\n",
        "* Summarize aged context to reduce token usage\n",
        "\n",
        "---\n",
        "\n",
        "### **B. Implementing LTM (4-Step Workflow)**\n",
        "\n",
        "To support cross-thread reasoning (e.g., “What did we try last time?”), LTM runs **alongside** the chat loop:\n",
        "\n",
        "1. **Creation (Extraction):**\n",
        "   Identify “Memory Candidates” from conversation; filter noise\n",
        "\n",
        "2. **Storage:**\n",
        "   Save structured memories to durable storage with metadata\n",
        "   (e.g., Vector Store, SQL, JSON)\n",
        "\n",
        "3. **Retrieval:**\n",
        "   Perform **Selective Search**, not exhaustive loading, for relevant facts\n",
        "\n",
        "4. **Injection:**\n",
        "   Inject retrieved memories into STM/context window before next LLM call\n",
        "\n",
        "This enables the LLM to **see past knowledge** as new tokens.\n",
        "\n",
        "---\n",
        "\n",
        "## **4. System Challenges**\n",
        "\n",
        "The video lists practical challenges in building LTM:\n",
        "\n",
        "1. **Context Window Overflow**\n",
        "\n",
        "   * STM grows without bound → summarization solves token pressure\n",
        "\n",
        "2. **Fragility**\n",
        "\n",
        "   * STM lost on server restart → persistent DB needed\n",
        "\n",
        "3. **Selection Problem**\n",
        "\n",
        "   * Hardest problem: deciding **what to save**\n",
        "   * saving everything → noisy retrieval\n",
        "   * saving too little → poor recall\n",
        "\n",
        "4. **Orchestration Complexity**\n",
        "\n",
        "   * manually building creation/storage/retrieval loops is complicated\n",
        "\n",
        "---\n",
        "\n",
        "## **5. Tools & Ecosystem**\n",
        "\n",
        "To reduce engineering cost, the instructor recommends:\n",
        "\n",
        "* **LangMem**\n",
        "* **Mem0**\n",
        "* **SuperMemory**\n",
        "\n",
        "These libraries automate parts of the 4-step LTM pipeline (extraction → storage → retrieval → injection).\n",
        "\n",
        "---\n",
        "\n",
        "## **6. Future Directions**\n",
        "\n",
        "Research like Google’s **Titans + Mirage** explores models with **Intrinsic Memory**, potentially eliminating external memory scaffolding and enabling:\n",
        "\n",
        "* writable internal states\n",
        "* multi-timescale memory hierarchies\n",
        "* cross-episode continuity without token replay\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "nNPvdEf4w76y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "E7UztQ2vxJwV"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}