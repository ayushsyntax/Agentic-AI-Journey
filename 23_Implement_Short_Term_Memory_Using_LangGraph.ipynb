{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUT59GHWXEcewXVD7qxkeI"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "w2HAd_9DovpE"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "## üìå **Short-Term Memory (STM) in LangGraph ‚Äî Practical Notes (Clean & Simple)**\n",
        "\n",
        "### **What Short-Term Memory Actually Is**\n",
        "\n",
        "Short-Term Memory in LangGraph stores the *current conversation history* (messages, state) so your AI agent remembers what happened earlier in the *same session*. This data is tied to a **thread ID** so the agent can resume where it left off if needed, even across restarts. LangGraph manages this as **state inside the graph**, and you persist it with a **checkpointer**. ([LangChain Docs][1])\n",
        "\n",
        "---\n",
        "\n",
        "## üß† **1. STM Persistence ‚Äî MemorySaver vs. PostgresSaver**\n",
        "\n",
        "Short-term memory defaults to RAM, which means **state is lost on restart**.\n",
        "\n",
        "### üëâ A. RAM (Volatile)\n",
        "\n",
        "* Uses `MemorySaver` or `InMemorySaver`\n",
        "* Good for development/testing\n",
        "* Conversation history is lost when the script stops\n",
        "\n",
        "### üëâ B. Production-ready Persistence\n",
        "\n",
        "* Uses **PostgreSQL** through `PostgresSaver`\n",
        "* Chat history persists across server restarts\n",
        "* Each thread‚Äôs state (messages + variables) is stored on disk\n",
        "\n",
        "**Example (Python)**\n",
        "\n",
        "```python\n",
        "from langgraph.checkpoint.postgres import PostgresSaver\n",
        "\n",
        "DB_URI = \"postgresql://user:pass@localhost:5432/postgres?sslmode=disable\"\n",
        "\n",
        "with PostgresSaver.from_conn_string(DB_URI) as checkpointer:\n",
        "    graph = builder.compile(checkpointer=checkpointer)\n",
        "    # Now Graph state survives process restarts\n",
        "```\n",
        "\n",
        "When you run a query later with the same `thread_id`, LangGraph loads back the saved state automatically. ([LangChain Docs][1])\n",
        "\n",
        "---\n",
        "\n",
        "## üõ† **2. Keep LLM Prompts Within Context Limits**\n",
        "\n",
        "LLMs have LIMITED context windows. If you dump huge history, you either crash the LLM or spend tokens you don‚Äôt need. Two solid strategies:\n",
        "\n",
        "### üîπ A. *Trimming* ‚Äî Keep only the recent context\n",
        "\n",
        "**Concept:** When the message list gets too long, keep only the last N tokens/messages.\n",
        "\n",
        "**Why:** LLMs don‚Äôt need all ancient conversation to respond to the next turn; usually the most recent entries matter most. ([LangChain Docs][2])\n",
        "\n",
        "**Simple Code Snippet**\n",
        "\n",
        "```python\n",
        "from langchain_core.messages import trim_messages\n",
        "\n",
        "def model_node(state):\n",
        "    messages = state[\"messages\"]\n",
        "\n",
        "    # Trim so total tokens ~150 (example)\n",
        "    trimmed = trim_messages(\n",
        "        messages,\n",
        "        max_tokens=150,\n",
        "        strategy=\"last\",\n",
        "        token_counter=len # or a proper tokenizer\n",
        "    )\n",
        "\n",
        "    response = model.invoke(trimmed)\n",
        "    return {\"messages\": trimmed + [response]}\n",
        "```\n",
        "\n",
        "**What Happens:** messages older than the desired token limit are dropped *before* calling LLM. ([LangChain Docs][2])\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ B. *Summarization* ‚Äî Compress old history\n",
        "\n",
        "**Concept:** Instead of throwing away old messages, compress them into a short **summary**, then carry on. This keeps *important context* without consuming many tokens. ([Medium][3])\n",
        "\n",
        "**Typical Workflow**\n",
        "\n",
        "1. Detect if `len(messages)` > threshold (e.g., 6)\n",
        "2. Call summarization LLM on oldest chunk\n",
        "3. Store summary in state\n",
        "4. Remove old messages using `RemoveMessage`\n",
        "\n",
        "```python\n",
        "from langchain_core.messages import HumanMessage\n",
        "from langgraph.graph.message import RemoveMessage\n",
        "\n",
        "def should_summarize(state):\n",
        "    msgs = state[\"messages\"]\n",
        "    return \"summarize_node\" if len(msgs) > 6 else END\n",
        "\n",
        "def summarize_node(state):\n",
        "    msgs_to_summarize = state[\"messages\"][:4]\n",
        "    summary = llm.invoke(msgs_to_summarize).content\n",
        "\n",
        "    # Remove old messages from state\n",
        "    remove_ops = [RemoveMessage(id=m.id) for m in msgs_to_summarize]\n",
        "    return {\n",
        "        \"summary\": summary,\n",
        "        \"messages\": remove_ops\n",
        "    }\n",
        "```\n",
        "\n",
        "**After summarization:**\n",
        "Later nodes add that summary back into the context so the next LLM call sees:\n",
        "`[summary, recent messages ...]`\n",
        "\n",
        "Summary workouts retain *coherence* without clogging the LLM‚Äôs context. ([Medium][3])\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ö†Ô∏è **3. Why This Matters (Problems Solved)**\n",
        "\n",
        "### Problem: Server Restarts ‚Üí ‚ÄúHello Stranger‚Äù\n",
        "\n",
        "If your bot keeps memory only in RAM, every restart will lose history.\n",
        "**Solution:** `PostgresSaver` (or any durable checkpointer). ([LangChain Docs][1])\n",
        "\n",
        "### Problem: Context Explosion\n",
        "\n",
        "Chat history balloons and crashes or slows LLM.\n",
        "**Solution 1:** Trimming = simple dropping. ([LangChain Docs][2])\n",
        "**Solution 2:** Summarization = keep the *gist*. ([Medium][3])\n",
        "\n",
        "### Problem: Infinite Growth\n",
        "\n",
        "Without explicit deletion, state grows indefinitely.\n",
        "**Solution:** Use `RemoveMessage` to actually delete old entries rather than ignoring them. ([LangChain Docs][2])\n",
        "\n",
        "---\n",
        "\n",
        "## üß™ **Quick Pseudocode for Runtime Flow**\n",
        "\n",
        "```pseudo\n",
        "on graph.invoke(inputs, thread_id):\n",
        "    state = load_state(thread_id)\n",
        "\n",
        "    if needs_summarize(state):\n",
        "        call summarize_node\n",
        "        persist summary\n",
        "\n",
        "    trimmed = trim_messages_if_too_large(state[\"messages\"])\n",
        "    llm_response = model.invoke(trimmed)\n",
        "\n",
        "    update state with llm_response\n",
        "    save_state(thread_id)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üñº Visual Summary (Images for Implementation Reference)\n",
        "\n",
        "![Image](https://miro.medium.com/1%2AqxX1LtmcyynbdS0ghRAlEQ.png)\n",
        "\n",
        "![Image](https://mintcdn.com/langchain-5e9cc07a/-_xGPoyjhyiDWTPJ/oss/images/checkpoints.jpg?auto=format\\&fit=max\\&n=-_xGPoyjhyiDWTPJ\\&q=85\\&s=966566aaae853ed4d240c2d0d067467c)\n",
        "\n",
        "![Image](https://blog.jetbrains.com/wp-content/uploads/2025/12/Context-management-strategies.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AZkUbqiRyceeCtl40VsGQ5A.jpeg)\n",
        "\n",
        "---\n",
        "\n",
        "## üìç Implementation Checklist (Simple)\n",
        "\n",
        "1. **Decide storage:**\n",
        "\n",
        "   * Dev: `MemorySaver`\n",
        "   * Prod: `PostgresSaver` (Docker PostgreSQL) ([LangChain Docs][1])\n",
        "\n",
        "2. **Track thread IDs:** Always pass a `thread_id` when invoking. ([LangChain Docs][1])\n",
        "\n",
        "3. **Manage context size:**\n",
        "\n",
        "   * Trim using `trim_messages`\n",
        "   * Summarize with a summarization LLM node\n",
        "   * Use `RemoveMessage` for cleanup ([LangChain Docs][2])\n",
        "\n",
        "4. **Persist state each turn:** Graph must save state after responders run so next invoke continues conversation. ([LangChain Docs][1])\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "9NWTQ99_qI68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "‚Äì‚Äì‚Äì\n",
        "\n",
        "Q: What does STM actually store?\n",
        "A: The current conversation thread: messages, summaries, and any state variables needed to produce the next response.\n",
        "\n",
        "Q: How is STM keyed?\n",
        "A: By `thread_id`. Without a stable thread identifier, there is no memory continuity.\n",
        "\n",
        "Q: Why does a server restart wipe memory when using default settings?\n",
        "A: Because `MemorySaver` keeps state in process memory only; it has no persistence layer.\n",
        "\n",
        "Q: What happens when you switch to `PostgresSaver`?\n",
        "A: State becomes durable. After restart, passing the same `thread_id` reconstructs the thread and resumes seamlessly.\n",
        "\n",
        "Q: Why worry about context size?\n",
        "A: Large histories quickly exceed LLM context windows, inflate cost, and induce failure modes (timeouts, hallucinations, truncation).\n",
        "\n",
        "Q: Why trimming instead of summarizing?\n",
        "A: Trimming is trivial to implement and fast. It retains recency but discards older context.\n",
        "\n",
        "Q: Why summarizing instead of trimming?\n",
        "A: Summarization preserves informational continuity: details from previous turns survive as compressed content.\n",
        "\n",
        "Q: Why explicit deletion?\n",
        "A: STM state is cumulative unless pruned. Without deletion operators like `RemoveMessage`, the underlying state will grow indefinitely‚Äîeven if older content is ignored during LLM calls.\n",
        "\n",
        "Q: Is summarization the same as long-term memory?\n",
        "A: No. Summarization is compression of short-term conversation context for the current thread. Long-term memory stores cross-thread user facts.\n",
        "\n",
        "‚Äì‚Äì‚Äì\n",
        "\n",
        "Key points to remember:\n",
        "\n",
        "‚Ä¢ STM is thread-scoped, not user-scoped\n",
        "‚Ä¢ Persistence is opt-in: RAM is volatile, Postgres is durable\n",
        "‚Ä¢ Token limits are real engineering constraints, not theory\n",
        "‚Ä¢ Summarization avoids ‚Äúcontext amnesia‚Äù while controlling size\n",
        "‚Ä¢ Explicit state mutation is mandatory for proper pruning\n",
        "‚Ä¢ Durable STM removes the ‚Äústranger problem‚Äù on server restarts\n",
        "‚Ä¢ Checkpointers are not optional if you want production reliability\n",
        "‚Ä¢ Thread IDs are the address space for STM\n",
        "‚Ä¢ Trimming optimizes for performance, summarization for coherence\n",
        "‚Ä¢ STM is prerequisite for long-term memory but not a replacement for it\n",
        "\n"
      ],
      "metadata": {
        "id": "PfegJGwRqdFK"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xOlVrcOdqNQr"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}