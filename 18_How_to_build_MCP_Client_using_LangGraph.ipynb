{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMjpNIHW7xVTW1OOe0sSC4W"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "## **MCP Architecture (High-Level Visual & Concepts)**\n",
        "\n",
        "![Image](https://static.wixstatic.com/media/e6ecb9_c4f0699a79544b189988da9e705afecb~mv2.jpg/v1/fill/w_980%2Ch_619%2Cal_c%2Cq_85%2Cusm_0.66_1.00_0.01%2Cenc_avif%2Cquality_auto/e6ecb9_c4f0699a79544b189988da9e705afecb~mv2.jpg)\n",
        "\n",
        "![Image](https://www.descope.com/_next/image?q=75\\&url=https%3A%2F%2Fimages.ctfassets.net%2Fxqb1f63q68s1%2F6R2RtSw84mTFLKfYBPpqNQ%2Ff1ef779c252dde2997f6cc1ab92fa794%2FMCP_general_architecture-min.png\\&w=1920)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1358/1%2AsSVwjNeVLHzZ6wou1TAtTg.png)\n",
        "\n",
        "![Image](https://www.tenable.com/sites/default/files/images/blog/90a9242f-116e-4a45-89d6-edb138f3c25f.png)\n",
        "\n",
        "![Image](https://workato.com/the-connector/wp-content/uploads/2025/04/AD_4nXeVNy0WC-VcqhWQGgZ9NsWtodJ7BM_n6WRUs58hQEAcNvCtrdtmNBr7_HYDK47TiZt0xM3uBA-14Fc0B4I-xpC8qotMD2LXPFBVIEZFkDfmhDRlvCOPCwCeXiAFNtcOEdx_zKTjkeyZfP96ADPBWnGE3_T44ny1JMJ.png)\n",
        "\n",
        "The **Model Context Protocol (MCP)** is a **standard, JSON-RPC based client-server protocol** that lets LLM applications (Clients/Hosts) discover and invoke external tools without embedding custom integrations per tool. It solves the *NÃ—M maintenance problem* by isolating **tools on servers** and exposing them through a consistent interface. ([Model Context Protocol][1])\n",
        "\n",
        "* **Clients/Hosts**: LLM workflows (LangGraph, agents, chatbots).\n",
        "* **Servers**: Standalone services exposing tools (math, expenses, APIs, data stores).\n",
        "* **Transport**: stdio, HTTP/SSE, socket JSON-RPC.\n",
        "* **Protocol**: JSON-RPC for method invocation, tool discovery, tool metadata, input validation. ([LangChain Docs][2])\n",
        "\n",
        "Below the surface, MCP resembles a â€œ**USB-C port for AI tools**â€: standardized at the protocol layer so any server can plug into any client. ([Medium][3])\n",
        "\n",
        "---\n",
        "\n",
        "## **Async Programming Fundamentals for MCP (Python)**\n",
        "\n",
        "MCP clients and servers require true **asynchronous IO**â€”the ability to do non-blocking waits and concurrency in a single thread using **async/await** (Python 3.7+). The async model runs an **event loop**, scheduling coroutines rather than blocking threads. ([Real Python][4])\n",
        "\n",
        "![Image](https://geekpython.in/wp-content/uploads/2023/08/async-await.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2AE_hYZrM8bccKFecUDlD8ng.png)\n",
        "\n",
        "![Image](https://cdn.hackersandslackers.com/2021/04/async_eventloop.jpg)\n",
        "\n",
        "Key Points:\n",
        "\n",
        "* **Event loop**: Schedules tasks and resumes them when IO completes. ([Real Python][4])\n",
        "* **async/await**: `async` declares a coroutine; `await` yields control until awaited task completes. ([Real Python][4])\n",
        "* Use **async frameworks** (FastAPI, trio, asyncio) for servers/clients.\n",
        "\n",
        "**Python async snippet (blocking vs async)**\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "\n",
        "async def slow_io(x: int) -> str:\n",
        "    # simulate network delay\n",
        "    await asyncio.sleep(1)\n",
        "    return f\"done:{x}\"\n",
        "\n",
        "async def main():\n",
        "    # schedule coroutines concurrently\n",
        "    results = await asyncio.gather(slow_io(1), slow_io(2))\n",
        "    print(results)\n",
        "\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## **Core Implementation Blueprint with LangGraph + MCP**\n",
        "\n",
        "### **1. Install Dependencies**\n",
        "\n",
        "```bash\n",
        "python3 -m venv venv && . venv/bin/activate\n",
        "pip install langchain langgraph langchain-mcp-adapters[all] fastmcp uvicorn\n",
        "```\n",
        "\n",
        "Pin versions in `requirements.txt`:\n",
        "\n",
        "```\n",
        "langchain==0.1.0\n",
        "langgraph==0.1.0\n",
        "langchain-mcp-adapters==0.1.0\n",
        "fastmcp==0.1.0\n",
        "uvicorn==0.23.0\n",
        "```\n",
        "\n",
        "Ensure TLS1.3+ in deployment (nginx/CLOUDFLARE + Uvicorn with SSL certs).\n",
        "\n",
        "---\n",
        "\n",
        "### **2. MCP Server (FastAPI + FastMCP)**\n",
        "\n",
        "This server exposes two tools (`add` and `subtract`) via MCP.\n",
        "\n",
        "```python\n",
        "# mcp_server.py\n",
        "import asyncio\n",
        "from fastmcp import FastMCP, tool\n",
        "\n",
        "mcp = FastMCP()\n",
        "\n",
        "@tool()\n",
        "async def add(a: float, b: float) -> float:\n",
        "    return a + b\n",
        "\n",
        "@tool()\n",
        "async def subtract(a: float, b: float) -> float:\n",
        "    return a - b\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run(transport=\"stdio\")  # JSON-RPC over stdio\n",
        "```\n",
        "\n",
        "* Tools are decorated with `@tool()` and use **async defs**.\n",
        "* Run via `python mcp_server.py`. This is the **local server** for MCP.\n",
        "* **Security**: Validate all inputs, enforce rate limits, and sign manifests for tool definitions (see academic threat models). ([arXiv][5])\n",
        "\n",
        "---\n",
        "\n",
        "### **3. LangGraph App (Client + MCP Integration)**\n",
        "\n",
        "Integrate MCP tools into a LangGraph workflow:\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from langgraph.graph import StateGraph\n",
        "\n",
        "# multi-server client\n",
        "client = MultiServerMCPClient()\n",
        "client.add_client(\n",
        "    name=\"math_server\",\n",
        "    transport=\"stdio\",\n",
        "    command=\"python\",\n",
        "    args=[\"./mcp_server.py\"]\n",
        ")\n",
        "\n",
        "async def build_graph(llm):\n",
        "    # connect to MCP servers\n",
        "    await client.connect()\n",
        "    mcp_tools = await client.get_tools()\n",
        "    \n",
        "    # bind tools to LLM\n",
        "    llm_with_tools = llm.bind_tools(mcp_tools)\n",
        "    \n",
        "    async def chat_node(state):\n",
        "        # tool invocation handled via MCP\n",
        "        return {\"messages\": [await llm_with_tools.ainvoke(state[\"messages\"])]}\n",
        "\n",
        "    graph = StateGraph(State)\n",
        "    graph.add_node(\"chat\", chat_node)\n",
        "    workflow = graph.compile()\n",
        "    return workflow\n",
        "\n",
        "async def main():\n",
        "    llm = ...  # initialize async LLM\n",
        "    app = await build_graph(llm)\n",
        "    result = await app.ainvoke({\"messages\": [(\"user\",\"Add 2 + 3\")]})\n",
        "    print(result)\n",
        "\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "Benefits:\n",
        "\n",
        "* **No hardcoded tool implementations**: The LLM binds tools dynamically. ([LangChain Docs][2])\n",
        "* **Async end-to-end**: All nodes use `await` and play nicely with the event loop. ([Real Python][4])\n",
        "\n",
        "---\n",
        "\n",
        "## **Production Deployment & Hardening**\n",
        "\n",
        "* **Backend**: FastAPI + Uvicorn + HTTPS (backend).\n",
        "* **Frontend**: React/Next.js with SSO/MFA.\n",
        "* **CI/CD**:\n",
        "\n",
        "  * Build + test MCP servers in GitHub Actions.\n",
        "  * Lint + type checks (mypy).\n",
        "  * Canary deploy + rollback.\n",
        "* **Observability**: Tracing (Opentelemetry), logs (structured JSON), SLOs for tool latency.\n",
        "* **Trust Boundaries**:\n",
        "\n",
        "  * Validate and sanitize all MCP inputs/outputs.\n",
        "  * PCI/PHI compliance if needed.\n",
        "  * Signature verificaiton of tool definitions to mitigate descriptor poisoning (see threat model). ([arXiv][5])\n",
        "\n",
        "---\n",
        "\n",
        "## **Interview-Level MCP + LangGraph Q&A**\n",
        "\n",
        "**Q:** *Why use MCP instead of built-in tools?*\n",
        "**A:** MCP standardizes integration so you avoid brittle, tightly coupled code; only servers need updates when APIs change. ([Model Context Protocol][1])\n",
        "\n",
        "**Q:** *Why must Python code be asynchronous for MCP?*\n",
        "**A:** MCP clients use async transports and event loops to manage concurrent tool calls without blocking. ([Real Python][4])\n",
        "\n",
        "**Q:** *How do you mitigate security risks like tool abuse or descriptor poisoning?*\n",
        "**A:** Sign tool manifests, vet descriptors in a CI pipeline, and limit invocation scopes. Academic research identifies classes of semantic attacks (e.g., tool poisoning) requiring layered defenses. ([arXiv][5])\n",
        "\n",
        "**Q:** *How to combine standard LangChain tools with MCP tools?*\n",
        "**A:** Fetch MCP tools and merge them into your LangChain tool list; LLM will treat them uniformly.\n",
        "\n",
        "**Q:** *How to integrate RAG with MCP workflows?*\n",
        "**A:** Add a vector database connector as an MCP server or incorporate RAG in the LLM graph before tool binding.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "6-Dm8LntNp-F"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# ğŸ§© **What MCP Solves**\n",
        "\n",
        "Standard LangChain/LangGraph tools are **embedded inside the application**, which creates two scaling failures:\n",
        "\n",
        "---\n",
        "\n",
        "## **1. API Change Failure**\n",
        "\n",
        "If an external service changes its API, such as:\n",
        "\n",
        "```\n",
        "/repos/{owner}/{repo}/pulls â†’ /repos/{owner}/{repo}/prs\n",
        "```\n",
        "\n",
        "then every chatbot using that tool breaks because the API logic is inside the chatbot codebase.\n",
        "\n",
        "---\n",
        "\n",
        "## **2. $N \\times M Maintenance Failure**\n",
        "\n",
        "If you have:\n",
        "\n",
        "* **M chatbots** = 5\n",
        "* **N tools/services** = 10\n",
        "\n",
        "Then you maintain:\n",
        "\n",
        "```\n",
        "5 Ã— 10 = 50 integration points\n",
        "```\n",
        "\n",
        "because each chatbot integrates each tool individually.\n",
        "\n",
        "MCP solves this by:\n",
        "\n",
        "* moving tool logic to a **Server**\n",
        "* letting chatbots act as **Clients**\n",
        "* standardizing the interface between them\n",
        "\n",
        "So updates happen once, centrally, not in every client.\n",
        "\n",
        "---\n",
        "\n",
        "# âš™ï¸ **Code Comparison**\n",
        "\n",
        "Below are the two models solving the **same problem**: retrieving GitHub PRs.\n",
        "\n",
        "---\n",
        "\n",
        "# âŒ **Without MCP (Hardcoded / Brittle)**\n",
        "\n",
        "Tool logic lives inside the chatbot application.\n",
        "\n",
        "```python\n",
        "@tool\n",
        "def get_pull_requests(owner: str, repo: str):\n",
        "    url = f\"https://api.github.com/repos/{owner}/{repo}/pulls\"\n",
        "    headers = {\"Authorization\": f\"Bearer {GITHUB_TOKEN}\"}\n",
        "    response = requests.get(url, headers=headers)\n",
        "    return response.json()\n",
        "\n",
        "llm_with_tools = llm.bind_tools([get_pull_requests])\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "| Property       | Result              |\n",
        "| -------------- | ------------------- |\n",
        "| Tool logic     | embedded in chatbot |\n",
        "| API versioning | breaks app          |\n",
        "| Auth & errors  | your responsibility |\n",
        "| Sharing tools  | copy/paste          |\n",
        "| Deployment     | tightly coupled     |\n",
        "| Scaling        | poor                |\n",
        "\n",
        "If the GitHub API changes, all chatbot apps must be patched and redeployed.\n",
        "\n",
        "---\n",
        "\n",
        "# âœ… **With MCP (Decoupled / Configurable)**\n",
        "\n",
        "Tool logic lives in a **GitHub MCP Server**. Chatbot only consumes tools.\n",
        "\n",
        "```python\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "\n",
        "client = MultiServerMCPClient()\n",
        "\n",
        "client.add_client(\n",
        "    name=\"github_server\",\n",
        "    transport=\"stdio\",\n",
        "    command=\"python\",\n",
        "    args=[\"./github_mcp_server.py\"]\n",
        ")\n",
        "\n",
        "async def build_graph():\n",
        "    await client.connect()\n",
        "\n",
        "    mcp_tools = await client.get_tools()  # auto-discover tools\n",
        "\n",
        "    llm_with_tools = llm.bind_tools(mcp_tools)\n",
        "```\n",
        "\n",
        "**Characteristics:**\n",
        "\n",
        "| Property      | Result              |\n",
        "| ------------- | ------------------- |\n",
        "| Tool logic    | external server     |\n",
        "| API changes   | server updates only |\n",
        "| Auth & errors | encapsulated        |\n",
        "| Sharing tools | plug-and-play       |\n",
        "| Deployment    | loosely coupled     |\n",
        "| Scaling       | excellent           |\n",
        "\n",
        "The chatbot doesnâ€™t care if the server uses REST, GraphQL, gRPC, v1 or v2.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ§± **System Roles**\n",
        "\n",
        "| Component      | Responsibility                  |\n",
        "| -------------- | ------------------------------- |\n",
        "| **MCP Server** | implements tools (GitHub logic) |\n",
        "| **MCP Client** | discovers + invokes tools       |\n",
        "| **LLM**        | decides when to call tools      |\n",
        "\n",
        "This separation mirrors modern microservice architecture:\n",
        "\n",
        "```\n",
        "LLM (client) â†’ MCP â†’ Tool Server (service)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# â™»ï¸ **Reusability Benefit**\n",
        "\n",
        "The **same GitHub MCP server** can be attached to:\n",
        "\n",
        "* LangGraph chatbot\n",
        "* Claude Desktop\n",
        "* VSCode\n",
        "* Cursor IDE\n",
        "* future MCP-compatible hosts\n",
        "\n",
        "without rewriting tool integrations.\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ—ï¸ **Why Async is Required**\n",
        "\n",
        "MCP uses asynchronous IO for:\n",
        "\n",
        "* server subprocess communication (stdio / SSE)\n",
        "* tool discovery\n",
        "* tool invocation\n",
        "* streaming outputs\n",
        "\n",
        "Therefore, LangGraph nodes must use:\n",
        "\n",
        "```python\n",
        "async def ...\n",
        "await ...\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸš¨ **Streamlit Note**\n",
        "\n",
        "The transcript correctly notes:\n",
        "\n",
        "* Streamlit = synchronous\n",
        "* MCP = asynchronous\n",
        "\n",
        "Result: requires â€œevent-loop hacksâ€\n",
        "Better production stack:\n",
        "\n",
        "* **Backend:** FastAPI (async-native)\n",
        "* **Frontend:** Next.js / React\n",
        "\n",
        "---\n",
        "\n",
        "# â›“ï¸ **Conclusion**\n",
        "\n",
        "MCP transforms tool usage from:\n",
        "\n",
        "```\n",
        "embedded logic â†’ external service\n",
        "```\n",
        "\n",
        "yielding:\n",
        "\n",
        "âœ“ lower coupling\n",
        "âœ“ centralized updates\n",
        "âœ“ better scale\n",
        "âœ“ interoperability across clients\n",
        "âœ“ version independence\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "eFthlvQsP54z"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "lAWV9kUyNxDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n",
        "# âœ”ï¸ **Case 1 â€” WITHOUT MCP (classic LangChain tool)**\n",
        "\n",
        "Here the math logic is **embedded inside the app**.\n",
        "\n",
        "### **math_tool.py**\n",
        "\n",
        "```python\n",
        "async def add(a: float, b: float) -> float:\n",
        "    return a + b\n",
        "```\n",
        "\n",
        "### **app_without_mcp.py**\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from langchain.tools import StructuredTool\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "class State(dict):\n",
        "    pass\n",
        "\n",
        "# define local tool\n",
        "add_tool = StructuredTool.from_function(add)\n",
        "\n",
        "async def main():\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    llm_with_tools = llm.bind_tools([add_tool])\n",
        "\n",
        "    async def chat(state: State):\n",
        "        out = await llm_with_tools.ainvoke(state[\"messages\"])\n",
        "        return {\"messages\": [out]}\n",
        "\n",
        "    graph = StateGraph(State)\n",
        "    graph.add_node(\"chat\", chat)\n",
        "    graph.add_edge(START, \"chat\")\n",
        "    app = graph.compile()\n",
        "\n",
        "    result = await app.ainvoke({\n",
        "        \"messages\": [(\"user\",\"add 4 and 7\")]\n",
        "    })\n",
        "    print(result)\n",
        "\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "### **Output**\n",
        "\n",
        "```\n",
        "{'messages': [{'role': 'assistant', 'content': '11'}]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# âŒ **Problems â€œWITHOUT MCPâ€**\n",
        "\n",
        "* Tool logic lives inside the app\n",
        "* If API changes â†’ you must edit app code\n",
        "* If 10 apps need the tool â†’ copy/paste hell\n",
        "* No versioning or discovery\n",
        "* No remote tools (only local Python)\n",
        "\n",
        "---\n",
        "\n",
        "# âœ”ï¸ **Case 2 â€” WITH MCP (standard protocol)**\n",
        "\n",
        "Now math logic moves to a **separate MCP server**.\n",
        "\n",
        "### **mcp_math_server.py**\n",
        "\n",
        "```python\n",
        "from fastmcp import FastMCP, tool\n",
        "\n",
        "mcp = FastMCP()\n",
        "\n",
        "@tool()\n",
        "async def add(a: float, b: float) -> float:\n",
        "    return a + b\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    mcp.run(transport=\"stdio\")\n",
        "```\n",
        "\n",
        "MCP server is standalone. Can run:\n",
        "\n",
        "```bash\n",
        "python mcp_math_server.py\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **app_with_mcp.py**\n",
        "\n",
        "```python\n",
        "import asyncio\n",
        "from langchain_mcp_adapters.client import MultiServerMCPClient\n",
        "from langgraph.graph import StateGraph, START\n",
        "from langchain.chat_models import ChatOpenAI\n",
        "\n",
        "class State(dict):\n",
        "    pass\n",
        "\n",
        "async def main():\n",
        "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
        "    client = MultiServerMCPClient()\n",
        "\n",
        "    client.add_client(\n",
        "        name=\"math\",\n",
        "        transport=\"stdio\",\n",
        "        command=\"python\",\n",
        "        args=[\"./mcp_math_server.py\"]\n",
        "    )\n",
        "\n",
        "    await client.connect()\n",
        "    tools = await client.get_tools()\n",
        "    llm_with_tools = llm.bind_tools(tools)\n",
        "\n",
        "    async def chat(state: State):\n",
        "        out = await llm_with_tools.ainvoke(state[\"messages\"])\n",
        "        return {\"messages\": [out]}\n",
        "\n",
        "    graph = StateGraph(State)\n",
        "    graph.add_node(\"chat\", chat)\n",
        "    graph.add_edge(START, \"chat\")\n",
        "    app = graph.compile()\n",
        "\n",
        "    result = await app.ainvoke({\n",
        "        \"messages\": [(\"user\",\"add 4 and 7\")]\n",
        "    })\n",
        "    print(result)\n",
        "\n",
        "asyncio.run(main())\n",
        "```\n",
        "\n",
        "### **Output**\n",
        "\n",
        "```\n",
        "{'messages': [{'role': 'assistant', 'content': '11'}]}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ§© **Side-by-Side Summary**\n",
        "\n",
        "| Feature            | Without MCP     | With MCP            |\n",
        "| ------------------ | --------------- | ------------------- |\n",
        "| Where tool lives   | inside app      | separate server     |\n",
        "| Tool update cost   | modify app code | update server only  |\n",
        "| Reuse across apps  | poor            | excellent           |\n",
        "| Versioning         | manual          | built-in            |\n",
        "| Discovery          | none            | yes (`get_tools()`) |\n",
        "| Transport          | function call   | JSON-RPC stdio/SSE  |\n",
        "| Remote support     | no              | yes                 |\n",
        "| Multiple tools     | copy/paste      | auto discover       |\n",
        "| Standard Interface | no              | yes (MCP)           |\n",
        "| DevOps             | minimal         | proper (server)     |\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ§  **Key Insight**\n",
        "\n",
        "With MCP your LLM app becomes **interoperable** â€” our new vocabulary gem again â€” meaning it can work with multiple tool sources without rebuilding itself.\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "TuJy5U2IOcyD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "# **1. Pre-built tools vs Custom tools â€” What changes?**\n",
        "\n",
        "### **Case A: Prebuilt LangChain Tools (No MCP)**\n",
        "\n",
        "Examples:\n",
        "\n",
        "* DuckDuckGoSearchTool\n",
        "* SQLDatabaseTool\n",
        "* VectorStoreRetrieverTool\n",
        "* WikipediaUtility\n",
        "* CalculatorTool\n",
        "\n",
        "These are **local Python tool classes** baked directly into your app.\n",
        "They do **not** use MCP.\n",
        "\n",
        "Used like:\n",
        "\n",
        "```python\n",
        "llm.bind_tools([DuckDuckGoSearchTool()])\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Case B: MCP Tools (Prebuilt or Custom)**\n",
        "\n",
        "These are tools exposed by **external servers** (local or remote), for example:\n",
        "\n",
        "* GitHub MCP Server\n",
        "* Google Drive MCP Server\n",
        "* Slack MCP Server\n",
        "* Calendar MCP Server\n",
        "* Math MCP Server (custom)\n",
        "* CRM MCP Server (enterprise)\n",
        "* Finance MCP Server (cloud)\n",
        "* RAG/Vector MCP Server\n",
        "\n",
        "These are accessed via:\n",
        "\n",
        "```python\n",
        "tools = await mcp_client.get_tools()\n",
        "llm.bind_tools(tools)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# **2. So what does MCP add?**\n",
        "\n",
        "It adds a **standard protocol** where tools can be:\n",
        "\n",
        "âœ“ remote\n",
        "âœ“ versioned\n",
        "âœ“ auto-discovered\n",
        "âœ“ shared between many apps\n",
        "âœ“ updated independently\n",
        "âœ“ polyglot (Node, Go, Rust, Python, etc)\n",
        "\n",
        "This canâ€™t be done with classic LangChain tools.\n",
        "\n",
        "---\n",
        "\n",
        "# **3. When MCP makes sense**\n",
        "\n",
        "MCP is most valuable when tools are:\n",
        "\n",
        "**âœ“ remote** (e.g., Google Calendar, CRM, ERP, Jira)\n",
        "**âœ“ shared by many agents** (avoid rewriting APIs)\n",
        "**âœ“ update frequently** (API versioning/hardening)\n",
        "**âœ“ owned by other teams** (backend team builds server)\n",
        "\n",
        "This matches enterprise patterns.\n",
        "\n",
        "---\n",
        "\n",
        "# **4. When MCP is overkill**\n",
        "\n",
        "If tools are:\n",
        "\n",
        "**âœ“ simple local helpers**\n",
        "(e.g., `add()`, `base64_encode()`, `parse_date()`)\n",
        "\n",
        "Using MCP adds no value â€” local Python tools are easier.\n",
        "\n",
        "---\n",
        "\n",
        "# **5. Blended Model â€” Works together**\n",
        "\n",
        "The cool part: LangGraph can bind **both at the same time**:\n",
        "\n",
        "```python\n",
        "local_tools  = [DuckDuckGoSearchTool()]\n",
        "mcp_tools    = await client.get_tools()\n",
        "\n",
        "llm_with_tools = llm.bind_tools(local_tools + mcp_tools)\n",
        "```\n",
        "\n",
        "So you donâ€™t choose one or the other â€” you combine both.\n",
        "\n",
        "---\n",
        "\n",
        "# **6. One more subtle point (important)**\n",
        "\n",
        "Most pre-built LangChain tools **could be converted** into MCP servers in the future.\n",
        "\n",
        "LangChain + LangGraph + Smith are trending toward:\n",
        "\n",
        "> **pre-built tools become MCP servers**\n",
        "\n",
        "Because it solves NÃ—M explosion for multi-agent architectures.\n",
        "\n",
        "---\n",
        "\n",
        "# **7. Real Production Implication**\n",
        "\n",
        "Your question implies this key distinction:\n",
        "\n",
        "> â€œIs MCP only for custom tools we write?â€\n",
        "\n",
        "**Answer:**\n",
        "No â€” MCP is a **delivery protocol**, not a â€œcustom tool mechanismâ€.\n",
        "Both prebuilt and custom tools can be delivered via MCP.\n",
        "\n",
        "---\n",
        "\n",
        "# **8. Example where MCP beats classic tools**\n",
        "\n",
        "Take Google Calendar:\n",
        "\n",
        "### Without MCP\n",
        "\n",
        "Every app must:\n",
        "\n",
        "* OAuth sign-in\n",
        "* manage refresh tokens\n",
        "* call Google API\n",
        "* parse responses\n",
        "* sanitize events\n",
        "* handle rate limits\n",
        "* handle timezones\n",
        "* re-deploy on scope changes\n",
        "\n",
        "### With MCP\n",
        "\n",
        "Apps just do:\n",
        "\n",
        "```\n",
        "schedule meeting at 4pm\n",
        "```\n",
        "\n",
        "Server handles all the rest.\n",
        "\n",
        "---\n",
        "\n",
        "# **9. Where the industry is heading**\n",
        "\n",
        "OpenAI, Anthropic, LangChain, ModelContextProtocol group are aligned toward:\n",
        "\n",
        "* **Agents plug into enterprise APIs via MCP bridges**\n",
        "* **LLMs not talking directly to APIs**\n",
        "* **Backend teams own servers**\n",
        "* **Frontend agents remain thin**\n",
        "\n",
        "Think **USB** for tools instead of soldering wires.\n",
        "\n",
        "---\n",
        "\n",
        "# **10. Final clarification in one line**\n",
        "\n",
        "> MCP is not about â€œcustom vs prebuilt toolsâ€,\n",
        "> itâ€™s about â€œembedded toolsâ€ vs â€œexternal standardized tool serversâ€.\n",
        "\n",
        "-"
      ],
      "metadata": {
        "id": "w9IPc3NzPIj_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MEaJg8TuOh-p"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}