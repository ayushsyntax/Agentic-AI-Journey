{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNUTXJ7zubUMJj7Pi1ElVJu"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "![Image](https://substackcdn.com/image/fetch/%24s_%21jnzp%21%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2Fc1eca5d1-0462-4833-ae12-7b645a4e3a20_1828x876.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AVIC4SGSBHMkZ9ktYdO5ApA.png)\n",
        "\n",
        "![Image](https://cdn.prod.website-files.com/640f56f76d313bbe39631bfd/67fe59e0c751ab61c59f32dd_customer%20support%20bot.png)\n",
        "\n",
        "## Stateful Chatbot with LangGraph — Clean, Step-by-Step Guide\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Conceptual Overview\n",
        "\n",
        "A basic LLM chatbot is *stateless*: each call processes only the current input.\n",
        "A **stateful chatbot** retains prior messages and uses them as context for future responses.\n",
        "\n",
        "LangGraph enables this by:\n",
        "\n",
        "* Defining an explicit **state schema**\n",
        "* Applying **reducers** to control how state evolves\n",
        "* Using **checkpointers** to persist state between invocations\n",
        "* Identifying conversations via a **thread_id**\n",
        "\n",
        "The workflow here is intentionally minimal: a **single-node sequential graph**.\n",
        "\n",
        "---\n",
        "\n",
        "## 2. State Definition\n",
        "\n",
        "```python\n",
        "from typing import TypedDict, Annotated\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "```\n",
        "\n",
        "```python\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "```\n",
        "\n",
        "### What this does\n",
        "\n",
        "* `messages` holds the full conversation history.\n",
        "* `BaseMessage` supports all LangChain message types:\n",
        "\n",
        "  * `HumanMessage`\n",
        "  * `AIMessage`\n",
        "  * `SystemMessage`\n",
        "  * `ToolMessage`\n",
        "* `add_messages` is a **LangGraph reducer** that:\n",
        "\n",
        "  * Appends new messages\n",
        "  * Prevents overwriting prior history\n",
        "\n",
        "This is the core mechanism that enables memory accumulation.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Node Logic\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "```\n",
        "\n",
        "```python\n",
        "model = ChatOpenAI(model=\"gpt-4o\")\n",
        "```\n",
        "\n",
        "```python\n",
        "def chat_node(state: ChatState):\n",
        "    response = model.invoke(state[\"messages\"])\n",
        "    return {\"messages\": [response]}\n",
        "```\n",
        "\n",
        "### Execution behavior\n",
        "\n",
        "* Receives the current `ChatState`\n",
        "* Sends the **entire message history** to the LLM\n",
        "* Returns the new AI message as a list\n",
        "* The reducer merges it into the existing state\n",
        "\n",
        "No mutation occurs inside the node; state evolution is declarative.\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Graph Construction\n",
        "\n",
        "```python\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "```\n",
        "\n",
        "```python\n",
        "builder = StateGraph(ChatState)\n",
        "\n",
        "builder.add_node(\"chat_node\", chat_node)\n",
        "builder.add_edge(START, \"chat_node\")\n",
        "builder.add_edge(\"chat_node\", END)\n",
        "```\n",
        "\n",
        "### Graph topology\n",
        "\n",
        "```\n",
        "START ──▶ chat_node ──▶ END\n",
        "```\n",
        "\n",
        "* Linear, deterministic execution\n",
        "* One node processes one conversational turn\n",
        "* State is finalized at `END`\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Persistence via Checkpointing\n",
        "\n",
        "```python\n",
        "from langgraph.checkpoint.memory import MemorySaver\n",
        "```\n",
        "\n",
        "```python\n",
        "memory = MemorySaver()\n",
        "chatbot = builder.compile(checkpointer=memory)\n",
        "```\n",
        "\n",
        "### What the checkpointer does\n",
        "\n",
        "* Automatically saves state at graph termination\n",
        "* Restores state on the next invocation\n",
        "* Uses in-memory storage (RAM-only)\n",
        "\n",
        "Without a checkpointer, the graph would forget all prior messages after each run.\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Thread-Scoped Conversation Memory\n",
        "\n",
        "```python\n",
        "config = {\"configurable\": {\"thread_id\": \"user_123\"}}\n",
        "```\n",
        "\n",
        "### Why `thread_id` matters\n",
        "\n",
        "* Acts as a **conversation key**\n",
        "* Allows multiple independent sessions\n",
        "* Ensures correct state retrieval for each user\n",
        "\n",
        "Same `thread_id` → same conversation history.\n",
        "\n",
        "---\n",
        "\n",
        "## 7. Runtime Execution Loop\n",
        "\n",
        "```python\n",
        "from langchain_core.messages import HumanMessage\n",
        "```\n",
        "\n",
        "```python\n",
        "def run_chat():\n",
        "    print(\"Chatbot started. Type 'quit' to exit.\")\n",
        "    while True:\n",
        "        user_input = input(\"User: \")\n",
        "        if user_input.lower() in [\"quit\", \"exit\", \"by\"]:\n",
        "            break\n",
        "\n",
        "        initial_state = {\n",
        "            \"messages\": [HumanMessage(content=user_input)]\n",
        "        }\n",
        "\n",
        "        output = chatbot.invoke(initial_state, config=config)\n",
        "        print(f\"AI: {output['messages'][-1].content}\")\n",
        "```\n",
        "\n",
        "### Execution flow per turn\n",
        "\n",
        "1. User input wrapped in `HumanMessage`\n",
        "2. LangGraph retrieves stored state via `thread_id`\n",
        "3. New message appended using `add_messages`\n",
        "4. Full history sent to the LLM\n",
        "5. AI response appended and persisted\n",
        "\n",
        "The user experiences continuous memory without manual state handling.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. End-to-End Lifecycle Summary\n",
        "\n",
        "1. **Input arrives** as a `HumanMessage`\n",
        "2. **State is restored** from memory (by `thread_id`)\n",
        "3. **Reducer appends** the new message\n",
        "4. **LLM processes** full conversation history\n",
        "5. **AI response appended**\n",
        "6. **Checkpoint saved** at `END`\n",
        "\n",
        "All persistence is automatic and transparent.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Key Takeaways\n",
        "\n",
        "* `add_messages` enables safe, append-only message history\n",
        "* `MemorySaver` provides session persistence\n",
        "* `thread_id` scopes memory to individual users\n",
        "* Graph structure remains minimal and deterministic\n",
        "* No global variables, no manual state mutation\n",
        "\n"
      ],
      "metadata": {
        "id": "yaaGoWI8Xmbh"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "kRjMMmwJXrMI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Interview Answers — LangGraph Stateful Chatbot\n",
        "\n",
        "**What problem does LangGraph solve compared to a basic LLM chatbot?**\n",
        "LangGraph provides explicit state management and deterministic workflows. It enables persistent, multi-turn conversations by modeling execution as a graph with controlled state transitions and reducers, rather than ad-hoc message passing.\n",
        "\n",
        "**What is the role of `ChatState` in this design?**\n",
        "`ChatState` defines the schema for shared state across nodes. It enforces type safety and makes state evolution explicit. In this case, it holds the full conversation history as a list of `BaseMessage`.\n",
        "\n",
        "**Why use `add_messages` instead of `operator.add`?**\n",
        "`add_messages` is a LangGraph-specific reducer optimized for message history. It appends new messages without overwriting prior ones and preserves correct message ordering and types.\n",
        "\n",
        "**How does persistence work in LangGraph?**\n",
        "Persistence is implemented via a checkpointer. After graph execution reaches `END`, the checkpointer stores the final state. On the next invocation, the state is automatically restored before execution continues.\n",
        "\n",
        "**What is `MemorySaver` and when would you use it?**\n",
        "`MemorySaver` is an in-memory checkpointer. It is suitable for local development, demos, or ephemeral sessions. For production, it is typically replaced with a database-backed checkpointer.\n",
        "\n",
        "**What is the purpose of `thread_id`?**\n",
        "`thread_id` scopes state to a specific conversation. It allows multiple concurrent users or sessions by mapping each invocation to the correct persisted state.\n",
        "\n",
        "**How does the LLM receive conversation context?**\n",
        "The entire message history stored in `ChatState[\"messages\"]` is passed to `model.invoke()`, ensuring responses are conditioned on all prior turns.\n",
        "\n",
        "**Why return `{\"messages\": [response]}` from the node?**\n",
        "Reducers expect incremental updates. Returning the response as a list allows `add_messages` to merge it into the existing state instead of replacing it.\n",
        "\n",
        "**What guarantees determinism in this workflow?**\n",
        "A linear graph topology, explicit state schema, pure node functions, and reducer-based state updates ensure predictable execution.\n",
        "\n",
        "**How would you scale this design?**\n",
        "By replacing `MemorySaver` with a persistent store, adding routing or tool nodes, and introducing conditional edges while keeping state and reducers explicit.\n",
        "\n",
        "**What is the biggest architectural benefit of LangGraph here?**\n",
        "Separation of concerns: conversation memory, execution flow, and model invocation are decoupled, making the system auditable, testable, and extensible.\n"
      ],
      "metadata": {
        "id": "AzNTsrx4Xvco"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftAHTXfeXwtR"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}