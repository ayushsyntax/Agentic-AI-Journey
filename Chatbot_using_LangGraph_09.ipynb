{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOfHWcFFvuvpWT3FVFSZ1BY"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zpd0pSZOsEr"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1152/1%2Al0NrPI8UeYQvJO6qM8eYyw.png)\n",
        "\n",
        "![Image](https://blog.langchain.com/content/images/2024/01/simple_multi_agent_diagram--1-.png)\n",
        "\n",
        "![Image](https://www.cognee.ai/content/blog/posts/from-demo-to-production-1/atkinson.png)\n",
        "\n",
        "![Image](https://substackcdn.com/image/fetch/%24s_%21S8W-%21%2Cf_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A%2F%2Fsubstack-post-media.s3.amazonaws.com%2Fpublic%2Fimages%2F39d8e835-39c2-4a05-9a70-e3589f15eed3_706x432.png)\n",
        "\n",
        "## Stateful Chatbot with LangGraph — Clean, Code-First Explanation\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## 1. The Core Idea\n",
        "\n",
        "A raw LLM call is **stateless**:\n",
        "\n",
        "* Each `.invoke()` sees only the current prompt\n",
        "* Previous messages are forgotten\n",
        "\n",
        "LangGraph adds **explicit state**, so every turn builds on the previous ones.\n",
        "\n",
        "Key mechanisms:\n",
        "\n",
        "* A **state schema** (`ChatState`)\n",
        "* A **reducer** (`add_messages`) to append messages\n",
        "* A **graph** that defines execution order\n",
        "\n",
        "---\n",
        "\n",
        "## 2. State Definition (Memory Container)\n",
        "\n",
        "```python\n",
        "from typing import TypedDict, Annotated\n",
        "from langchain_core.messages import BaseMessage\n",
        "from langgraph.graph.message import add_messages\n",
        "```\n",
        "\n",
        "```python\n",
        "class ChatState(TypedDict):\n",
        "    messages: Annotated[list[BaseMessage], add_messages]\n",
        "```\n",
        "\n",
        "### What this means\n",
        "\n",
        "* `ChatState` defines **all shared memory**\n",
        "* `messages` holds the full conversation history\n",
        "* `BaseMessage` supports:\n",
        "\n",
        "  * `HumanMessage`\n",
        "  * `AIMessage`\n",
        "  * `SystemMessage`\n",
        "  * `ToolMessage`\n",
        "* `add_messages` is a **reducer**:\n",
        "\n",
        "  * Appends new messages\n",
        "  * Prevents overwriting existing history\n",
        "\n",
        "Without this reducer, each run would erase previous messages.\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Model Initialization\n",
        "\n",
        "```python\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "```\n",
        "\n",
        "* The model is created **once**\n",
        "* No state is stored inside the model\n",
        "* All memory lives in `ChatState`\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Chat Node (Pure Logic)\n",
        "\n",
        "```python\n",
        "def chat_node(state: ChatState):\n",
        "\n",
        "    # take user query from state\n",
        "    messages = state['messages']\n",
        "\n",
        "    # send to llm\n",
        "    response = llm.invoke(messages)\n",
        "\n",
        "    # response store state\n",
        "    return {'messages': [response]}\n",
        "```\n",
        "\n",
        "### Execution rules\n",
        "\n",
        "* Input: current `ChatState`\n",
        "* Uses **entire message history**\n",
        "* Returns **only the new AI message**\n",
        "* No mutation inside the function\n",
        "* State updates happen via reducers\n",
        "\n",
        "This makes the node deterministic and testable.\n",
        "\n",
        "---\n",
        "\n",
        "## 5. Graph Construction\n",
        "\n",
        "```python\n",
        "from langgraph.graph import StateGraph, START, END\n",
        "```\n",
        "\n",
        "```python\n",
        "graph = StateGraph(ChatState)\n",
        "\n",
        "# add nodes\n",
        "graph.add_node('chat_node', chat_node)\n",
        "\n",
        "graph.add_edge(START, 'chat_node')\n",
        "graph.add_edge('chat_node', END)\n",
        "```\n",
        "\n",
        "### Graph topology\n",
        "\n",
        "```\n",
        "START ──▶ chat_node ──▶ END\n",
        "```\n",
        "\n",
        "* Single-node, linear workflow\n",
        "* One execution = one conversation turn\n",
        "* State is finalized at `END`\n",
        "\n",
        "---\n",
        "\n",
        "## 6. Compile the Graph\n",
        "\n",
        "```python\n",
        "chatbot = graph.compile()\n",
        "chatbot\n",
        "```\n",
        "\n",
        "* Converts the graph definition into a runnable object\n",
        "* At this stage:\n",
        "\n",
        "  * No persistence\n",
        "  * Memory exists only during one invocation\n",
        "\n",
        "---\n",
        "\n",
        "## 7. First Invocation (Stateless Example)\n",
        "\n",
        "```python\n",
        "from langchain_core.messages import HumanMessage\n",
        "\n",
        "initial_state = {\n",
        "    'messages': [HumanMessage(content='What is the capital of india')]\n",
        "}\n",
        "```\n",
        "\n",
        "```python\n",
        "chatbot.invoke(initial_state)['messages'][-1].content\n",
        "```\n",
        "\n",
        "Output:\n",
        "\n",
        "```\n",
        "'New Delhi'\n",
        "```\n",
        "\n",
        "### What happened internally\n",
        "\n",
        "1. `HumanMessage` enters the state\n",
        "2. `chat_node` receives the state\n",
        "3. Full message list is sent to the LLM\n",
        "4. AI response is appended via `add_messages`\n",
        "5. Graph reaches `END`\n",
        "6. State is returned\n",
        "\n",
        "After this, **memory is lost** because no checkpointer is attached.\n",
        "\n",
        "---\n",
        "\n",
        "## 8. Why This Becomes Stateful with Persistence\n",
        "\n",
        "To make this chatbot remember previous turns, you add a **checkpointer** and a **thread_id**.\n",
        "\n",
        "Conceptually:\n",
        "\n",
        "```\n",
        "thread_id ──▶ stored ChatState ──▶ next invoke\n",
        "```\n",
        "\n",
        "* Same `thread_id` → same conversation\n",
        "* Different `thread_id` → isolated session\n",
        "\n",
        "The rest of your code remains unchanged.\n",
        "\n",
        "---\n",
        "\n",
        "## 9. Mental Model (One Sentence)\n",
        "\n",
        "LangGraph works by **restoring state → appending new messages → running deterministic nodes → saving updated state**.\n",
        "\n",
        "No globals. No hidden memory. No side effects.\n",
        "\n",
        "---\n",
        "\n",
        "## 10. Key Takeaways\n",
        "\n",
        "* `ChatState` defines **what memory exists**\n",
        "* `add_messages` defines **how memory grows**\n",
        "* Nodes are **pure functions**\n",
        "* Graph defines **execution order**\n",
        "* Persistence is orthogonal and pluggable\n",
        "* Your code already follows production-grade structure\n",
        "\n",
        "---\n",
        "\n",
        "![Image](https://www.researchgate.net/publication/362616055/figure/fig1/AS%3A11431281079759140%401660854110416/Chatbot-state-flowchart.png)\n",
        "\n",
        "![Image](https://cdn.sanity.io/images/k7elabj6/production/3725900182de61769dc7bb325aa354616023ad94-1536x1024.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1400/1%2AA7n3IbjpHqHwNifWrWEbKg.png)\n",
        "\n",
        "![Image](https://miro.medium.com/v2/resize%3Afit%3A1200/1%2AnnFyglgG4QoF_GPouldYBg.png)\n",
        "\n"
      ],
      "metadata": {
        "id": "CBAdAIF9bqqF"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "SQLn0Ys3b5jY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Key Interview Answers — LangGraph Stateful Chatbot\n",
        "\n",
        "**What problem does LangGraph solve compared to a basic LLM chatbot?**\n",
        "LangGraph provides explicit, deterministic state management. Instead of passing message history manually, it models execution as a graph with a defined state schema and reducers, enabling reliable multi-turn conversations.\n",
        "\n",
        "**What is `ChatState` and why is it required?**\n",
        "`ChatState` defines the shared state schema for the graph. It enforces structure and type safety and makes all mutable data—such as conversation history—explicit and auditable.\n",
        "\n",
        "**Why is `add_messages` used instead of overwriting the message list?**\n",
        "`add_messages` is a reducer that appends new messages to existing state. It prevents accidental loss of history and guarantees correct ordering and message typing across turns.\n",
        "\n",
        "**How does the chatbot become stateful?**\n",
        "Statefulness comes from combining a state schema (`ChatState`), reducers (`add_messages`), and a checkpointer. Together, they allow state to persist across graph invocations.\n",
        "\n",
        "**What happens inside the `chat_node`?**\n",
        "The node receives the current state, sends the entire message history to the LLM, and returns only the new AI message. State mutation is handled externally by reducers.\n",
        "\n",
        "**Why does the node return `{\"messages\": [response]}` instead of a single message?**\n",
        "Reducers operate on collections. Returning a list allows `add_messages` to merge the new AI message into the existing message history instead of replacing it.\n",
        "\n",
        "**What guarantees determinism in this design?**\n",
        "A fixed graph topology, pure node functions, explicit state transitions, and reducer-based updates ensure predictable execution for each invocation.\n",
        "\n",
        "**What is the role of `START` and `END` nodes?**\n",
        "`START` defines where execution begins, and `END` defines where it terminates. Reaching `END` signals that state is finalized and ready for persistence.\n",
        "\n",
        "**Why is this approach better than using global variables or manual memory handling?**\n",
        "Global variables are error-prone and non-scalable. LangGraph’s state model is explicit, testable, thread-safe, and suitable for concurrent users.\n",
        "\n",
        "**How would you add memory across multiple user sessions?**\n",
        "By attaching a checkpointer and passing a unique `thread_id` with each invocation. The `thread_id` maps each user or session to its own persisted state.\n",
        "\n",
        "**How would this scale to production?**\n",
        "Replace in-memory checkpointing with a database-backed checkpointer, add conditional routing or tool nodes, and keep state evolution explicit through reducers.\n",
        "\n",
        "**What is the main architectural advantage of LangGraph in interviews?**\n",
        "Clear separation of concerns: execution flow, state management, and model inference are decoupled, making the system maintainable, auditable, and extensible.\n"
      ],
      "metadata": {
        "id": "8Y7onjBlcJf9"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "LS3jDl5McKh0"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}