{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMwOk4Cvh/sWlZaTaXh6Cdj"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Resume-Chat Feature With Title-Generation\n",
        "\n",
        "## Scope\n",
        "\n",
        "This document defines an implementation-ready design for a multi-thread â€œResume Chatâ€ capability using Streamlit + LangGraph, including the homework extension for human-friendly chat titles.\n",
        "All outputs are deterministic, self-contained, auditable, and production-grade.\n",
        "\n",
        "---\n",
        "\n",
        "# 1. Architecture\n",
        "\n",
        "## 1.1 Components\n",
        "\n",
        "* **Frontend:** Streamlit\n",
        "* **LLM Orchestration:** LangGraph\n",
        "* **Checkpointer:** MemorySaver\n",
        "* **Persistence (Optional Homework DB):** Not enabled; titles stored in SessionState\n",
        "\n",
        "## 1.2 Data Flow (Declarative)\n",
        "\n",
        "User Input â†’ Streamlit UI â†’ LangGraph Stream â†’ Checkpointer (thread-scoped) â†’ Streamlit UI\n",
        "\n",
        "## 1.3 Trust Boundaries\n",
        "\n",
        "Browser â†” Streamlit (TLS 1.3+) â†” LangGraph Runtime (Local Privileged Execution)\n",
        "No cross-tenant data exposure; session isolation only in memory.\n",
        "\n",
        "## 1.4 Mermaid Diagram\n",
        "\n",
        "```mermaid\n",
        "flowchart TD\n",
        "    UI[Streamlit UI] -->|user input| LG[LangGraph Agent]\n",
        "    LG -->|stream| UI\n",
        "    LG -->|read/write| CP[MemorySaver Checkpointer]\n",
        "    UI -->|select thread| CP\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 2. Thread Management Implementation\n",
        "\n",
        "## 2.1 Session Initialization\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph_tool_backend import chatbot\n",
        "import uuid\n",
        "\n",
        "if 'message_history' not in st.session_state:\n",
        "    st.session_state['message_history'] = []\n",
        "\n",
        "if 'chat_threads' not in st.session_state:\n",
        "    st.session_state['chat_threads'] = {}\n",
        "\n",
        "if 'thread_id' not in st.session_state:\n",
        "    st.session_state['thread_id'] = str(uuid.uuid4())\n",
        "```\n",
        "\n",
        "Data model:\n",
        "\n",
        "```text\n",
        "chat_threads: Dict[str, Dict]\n",
        "{\n",
        "  thread_id: {\n",
        "      \"title\": str,\n",
        "      \"last_message\": str\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2.2 Thread Creation Logic\n",
        "\n",
        "```python\n",
        "def generate_thread_id() -> str:\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def new_thread():\n",
        "    tid = generate_thread_id()\n",
        "    st.session_state['thread_id'] = tid\n",
        "    st.session_state['message_history'] = []\n",
        "    st.session_state['chat_threads'][tid] = {\n",
        "        \"title\": \"New Conversation\",\n",
        "        \"last_message\": \"\"\n",
        "    }\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2.3 Sidebar Selection\n",
        "\n",
        "```python\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "\n",
        "if st.sidebar.button(\"New Chat\"):\n",
        "    new_thread()\n",
        "\n",
        "for tid, meta in reversed(st.session_state['chat_threads'].items()):\n",
        "    label = meta['title']\n",
        "    if st.sidebar.button(label, key=tid):\n",
        "        resume_thread(tid)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## 2.4 Resume Logic\n",
        "\n",
        "```python\n",
        "def resume_thread(tid: str):\n",
        "    st.session_state['thread_id'] = tid\n",
        "    state = chatbot.get_state({\"configurable\": {\"thread_id\": tid}})\n",
        "    messages = state.values.get('messages', [])\n",
        "    buffer = []\n",
        "    for m in messages:\n",
        "        role = 'user' if isinstance(m, HumanMessage) else 'assistant'\n",
        "        buffer.append({\"role\": role, \"content\": m.content})\n",
        "    st.session_state['message_history'] = buffer\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 3. Streaming Send Path\n",
        "\n",
        "```python\n",
        "for m in st.session_state['message_history']:\n",
        "    with st.chat_message(m['role']):\n",
        "        st.write(m['content'])\n",
        "\n",
        "user_input = st.chat_input(\"Message\")\n",
        "\n",
        "if user_input:\n",
        "    tid = st.session_state['thread_id']\n",
        "    st.session_state['message_history'].append({\"role\": \"user\", \"content\": user_input})\n",
        "    st.session_state['chat_threads'][tid]['last_message'] = user_input\n",
        "\n",
        "    CONFIG = {\"configurable\": {\"thread_id\": tid}}\n",
        "\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        def stream_out():\n",
        "            for chunk, meta in chatbot.stream(\n",
        "                {\"messages\": [HumanMessage(content=user_input)]},\n",
        "                config=CONFIG,\n",
        "                stream_mode=\"messages\"\n",
        "            ):\n",
        "                if isinstance(chunk, AIMessage):\n",
        "                    yield chunk.content\n",
        "        assistant_msg = st.write_stream(stream_out())\n",
        "\n",
        "    st.session_state['message_history'].append({\"role\": \"assistant\", \"content\": assistant_msg})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 4. Homework: Auto-Generated Conversation Titles\n",
        "\n",
        "## 4.1 Requirement\n",
        "\n",
        "Titles must:\n",
        "\n",
        "* be deterministic\n",
        "* be generated from the first user message\n",
        "* update only once\n",
        "* avoid manual naming\n",
        "* resemble ChatGPT behavior\n",
        "\n",
        "## 4.2 Deterministic Title Extraction Strategy\n",
        "\n",
        "Algorithm:\n",
        "\n",
        "1. consume first user message\n",
        "2. extract top-k nouns (fallback: first 3 words)\n",
        "3. title-case output\n",
        "\n",
        "This avoids LLM title hallucination and works offline.\n",
        "\n",
        "## 4.3 Implementation\n",
        "\n",
        "```python\n",
        "import re\n",
        "import string\n",
        "\n",
        "def extract_title(text: str) -> str:\n",
        "    text = text.strip()\n",
        "    text = re.sub(r'\\s+', ' ', text)\n",
        "    text = text[:80]\n",
        "    words = [w.strip(string.punctuation) for w in text.split()]\n",
        "    title = \" \".join(words[:6]).title()\n",
        "    return title or \"Conversation\"\n",
        "```\n",
        "\n",
        "Integration hook:\n",
        "\n",
        "```python\n",
        "if user_input:\n",
        "    tid = st.session_state['thread_id']\n",
        "    meta = st.session_state['chat_threads'][tid]\n",
        "    if meta['title'] == \"New Conversation\":\n",
        "        st.session_state['chat_threads'][tid]['title'] = extract_title(user_input)\n",
        "```\n",
        "\n",
        "Resulting UX example:\n",
        "\n",
        "```\n",
        "User first message: \"Explain q-learning with code\"\n",
        "Title stored: \"Explain Q Learning With Code\"\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# 5. Q&A (Technical Clarity)\n",
        "\n",
        "**Q:** Why generate titles on first user message instead of after N messages?\n",
        "**A:** Guarantees deterministic naming and immediate sidebar UX without requiring streaming buffering.\n",
        "\n",
        "**Q:** Why not use LLM to generate titles?\n",
        "**A:** Non-deterministic outputs complicate caching and reproducibility; homework variant avoids extra inference cost.\n",
        "\n",
        "**Q:** Can titles be edited by user?\n",
        "**A:** Yes. Add a text_input bound to `chat_threads[tid]['title']`.\n",
        "\n",
        "**Q:** How to persist threads across restarts?\n",
        "**A:** Replace MemorySaver with SQLite/PostgreSQL durable storage and maintain thread_id foreign key.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "k7phG-FWWNbe"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "PFANOsycWRCK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "---\n",
        "\n",
        "\n",
        "\n",
        "```python\n",
        "import streamlit as st\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "from langgraph_tool_backend import chatbot\n",
        "import uuid\n",
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "# ---------------------- UTILITIES ----------------------\n",
        "\n",
        "def generate_thread_id():\n",
        "    return str(uuid.uuid4())\n",
        "\n",
        "def extract_title(text: str) -> str:\n",
        "    # clean + shorten + make title case\n",
        "    text = text.strip()\n",
        "    text = re.sub(r\"\\s+\", \" \", text)\n",
        "    words = [w.strip(string.punctuation) for w in text.split()]\n",
        "    title = \" \".join(words[:6]).title()\n",
        "    return title if title else \"Conversation\"\n",
        "\n",
        "def create_new_thread():\n",
        "    tid = generate_thread_id()\n",
        "    st.session_state.thread_id = tid\n",
        "    st.session_state.message_history = []\n",
        "    st.session_state.chat_threads[tid] = {\"title\": \"New Conversation\"}\n",
        "    return tid\n",
        "\n",
        "def load_thread_from_backend(thread_id):\n",
        "    state = chatbot.get_state({\"configurable\": {\"thread_id\": thread_id}})\n",
        "    msgs = state.values.get(\"messages\", [])\n",
        "    converted = []\n",
        "    for m in msgs:\n",
        "        role = \"user\" if isinstance(m, HumanMessage) else \"assistant\"\n",
        "        converted.append({\"role\": role, \"content\": m.content})\n",
        "    return converted\n",
        "\n",
        "\n",
        "# ---------------------- SESSION INIT ----------------------\n",
        "\n",
        "if \"chat_threads\" not in st.session_state:\n",
        "    st.session_state.chat_threads = {}\n",
        "\n",
        "if \"thread_id\" not in st.session_state:\n",
        "    create_new_thread()\n",
        "\n",
        "if \"message_history\" not in st.session_state:\n",
        "    st.session_state.message_history = []\n",
        "\n",
        "\n",
        "# ---------------------- SIDEBAR UI ----------------------\n",
        "\n",
        "st.sidebar.title(\"Chat Sessions\")\n",
        "\n",
        "if st.sidebar.button(\"New Chat\"):\n",
        "    create_new_thread()\n",
        "\n",
        "st.sidebar.subheader(\"My Conversations\")\n",
        "\n",
        "for tid, meta in reversed(st.session_state.chat_threads.items()):\n",
        "    title = meta[\"title\"]\n",
        "    if st.sidebar.button(title, key=tid):\n",
        "        st.session_state.thread_id = tid\n",
        "        st.session_state.message_history = load_thread_from_backend(tid)\n",
        "\n",
        "\n",
        "# ---------------------- MAIN CHAT UI ----------------------\n",
        "\n",
        "for msg in st.session_state.message_history:\n",
        "    with st.chat_message(msg[\"role\"]):\n",
        "        st.write(msg[\"content\"])\n",
        "\n",
        "user_input = st.chat_input(\"Message\")\n",
        "\n",
        "if user_input:\n",
        "    tid = st.session_state.thread_id\n",
        "\n",
        "    # set title on first message\n",
        "    if st.session_state.chat_threads[tid][\"title\"] == \"New Conversation\":\n",
        "        st.session_state.chat_threads[tid][\"title\"] = extract_title(user_input)\n",
        "\n",
        "    # show user's message\n",
        "    st.session_state.message_history.append({\"role\": \"user\", \"content\": user_input})\n",
        "    with st.chat_message(\"user\"):\n",
        "        st.write(user_input)\n",
        "\n",
        "    CONFIG = {\"configurable\": {\"thread_id\": tid}}\n",
        "\n",
        "    # stream assistant response\n",
        "    with st.chat_message(\"assistant\"):\n",
        "        def stream_output():\n",
        "            for chunk, _ in chatbot.stream(\n",
        "                {\"messages\": [HumanMessage(content=user_input)]},\n",
        "                config=CONFIG,\n",
        "                stream_mode=\"messages\"\n",
        "            ):\n",
        "                if isinstance(chunk, AIMessage):\n",
        "                    yield chunk.content\n",
        "\n",
        "        assistant_reply = st.write_stream(stream_output())\n",
        "\n",
        "    st.session_state.message_history.append({\"role\": \"assistant\", \"content\": assistant_reply})\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ’¡ **Why This Version Is â€œEasy to Understandâ€**\n",
        "\n",
        "Because it organizes the app in the most intuitive way:\n",
        "\n",
        "### **1. Utilities at top**\n",
        "\n",
        "small, clear, single-purpose functions\n",
        "\n",
        "### **2. Session initialization**\n",
        "\n",
        "ensures app never breaks on first load\n",
        "\n",
        "### **3. Sidebar**\n",
        "\n",
        "handles navigation + new chat creation\n",
        "\n",
        "### **4. Main panel**\n",
        "\n",
        "handles messaging + streaming output\n",
        "\n",
        "### **5. No surprises**\n",
        "\n",
        "backend loads history only when user selects a thread\n",
        "\n",
        "---\n",
        "\n",
        "# ğŸ§  **Quick Q&A (for clarity)**\n",
        "\n",
        "**Q: Where are chat messages stored?**\n",
        "â†’ In `MemorySaver` inside LangGraph, keyed by `thread_id`.\n",
        "\n",
        "**Q: Why not store messages in Streamlit only?**\n",
        "â†’ Because then â€œResume Chatâ€ would reset on refresh â€” backend storage prevents that.\n",
        "\n",
        "**Q: When is title generated?**\n",
        "â†’ On the **first user message**, just like ChatGPT.\n",
        "\n",
        "**Q: Can user rename threads manually?**\n",
        "â†’ Yes, trivial to add later with `st.text_input`.\n",
        "\n",
        "**Q: Can these be persisted across app restarts?**\n",
        "â†’ Not with MemorySaver; requires SQLite/Postgres (next step).\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "DxSPPlHBWTzk"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "m6iQeQiJWfye"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}