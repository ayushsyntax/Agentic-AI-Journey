{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPcpfD00eKZwSjVbhm/TfQC"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "\n",
        "![Image](https://www.geeky-gadgets.com/wp-content/uploads/2025/05/langsmith-language-model-development-tools_optimized.jpg)\n",
        "\n",
        "![Image](https://cdn.analyticsvidhya.com/wp-content/uploads/2025/10/image_5.webp)\n",
        "\n",
        "![Image](https://langsmith.langchain.ac.cn/assets/images/langgraph_with_langchain_trace-fc850a609ceda555dafb450e4176cfea.png)\n",
        "\n",
        "![Image](https://langsmith.langchain.ac.cn/assets/images/project-9fc0692079f84a1df9fdabb89add8652.png)\n",
        "\n",
        "![Image](https://substackcdn.com/image/fetch/f_auto%2Cq_auto%3Agood%2Cfl_progressive%3Asteep/https%3A//substack-post-media.s3.amazonaws.com/public/images/71642bdb-5078-466d-9e8f-aa3527cb3df3_2000x1144.png)\n",
        "\n",
        "![Image](https://cdn.prod.website-files.com/65b8cd72835ceeacd4449a53/65cd453536f303763069e534_Langsmith%20asset.webp)\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "\n",
        "## LangSmith + LangGraph Observability (Cleaned & Enhanced)\n",
        "\n",
        "LangSmith integration with LangGraph provides **Observability**, letting you trace the end-to-end execution of a chatbot. This is more than simple logs: you get structured performance data, token costs, latency, and internal node-level behavior. ([LangChain][1])\n",
        "\n",
        "### Why Observability Matters\n",
        "\n",
        "Observability gives you actionable visibility into your system. You can:\n",
        "\n",
        "• see *every message in and out* of your bot\n",
        "• understand *which graph nodes ran*\n",
        "• measure *token usage and latency*\n",
        "• inspect *errors, retrievers, LLM calls, and tool invocations* in detail\n",
        "\n",
        "This is critical for debugging and production readiness. ([LangChain][1])\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Core Configuration\n",
        "\n",
        "Before anything else, create a LangSmith account at `smith.langchain.com` and generate an **API key**. Then populate these environment variables to enable automatic tracing:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"your_api_key_here\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chatbot_Project\"\n",
        "```\n",
        "\n",
        "This config tells LangSmith to collect traces from your LangGraph runs and bucket them under the project name you choose. ([LangChain Docs][2])\n",
        "\n",
        "---\n",
        "\n",
        "## 2. Traces and Runs — What You Actually Get\n",
        "\n",
        "In LangSmith:\n",
        "\n",
        "**Trace** = one conversational turn (input → full graph execution → output)\n",
        "**Run** = a specific step in that trace (like a node invocation or LLM call)\n",
        "\n",
        "Each run logs:\n",
        "\n",
        "* exact text inputs and outputs\n",
        "* *token counts* (input + output)\n",
        "* *latency* and *time-to-first-token*\n",
        "* child runs in the execution tree\n",
        "\n",
        "This lets you reconstruct *every piece* of what the chatbot did behind the scenes. ([LangChain Docs][3])\n",
        "\n",
        "---\n",
        "\n",
        "## 3. Thread Organization for Conversations\n",
        "\n",
        "By default, LangSmith puts all traces in one long list. To turn that into actual threads (e.g., separate chats per user), you attach thread metadata:\n",
        "\n",
        "```python\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": st.session_state.thread_id\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"thread_id\": st.session_state.thread_id\n",
        "    },\n",
        "    \"run_name\": \"Chat_Turn\"\n",
        "}\n",
        "\n",
        "events = chatbot.stream(\n",
        "    {\"messages\": [HumanMessage(content=user_input)]},\n",
        "    config=config,\n",
        "    stream_mode=\"values\"\n",
        ")\n",
        "```\n",
        "\n",
        "This makes the LangSmith UI show *distinct threads* (like separate chats), rather than one flat log. ([LangChain Docs][2])\n",
        "\n",
        "---\n",
        "\n",
        "## 4. Key Benefits for Production\n",
        "\n",
        "Observability isn’t just an optional developer toy — it’s essential for shipping robust systems:\n",
        "\n",
        "**Visual Debugging**\n",
        "Trace execution visually, dive into slow nodes, missing context, or hidden token drains.\n",
        "\n",
        "**Threaded Conversations**\n",
        "View clean histories per session, not one global log.\n",
        "\n",
        "**Deep Feature Insight**\n",
        "RAG, tools, MCP, retrievers, and complex graph logic become inspectable and quantifiable.\n",
        "\n",
        "**LLMOps Tools**\n",
        "Includes dashboards, prompt playgrounds, dataset exporters, and evaluation engines. ([LangChain Docs][4])\n",
        "\n",
        "---\n",
        "\n",
        "## Q&A for Common Observability Concerns\n",
        "\n",
        "**Q: Will LangSmith change how my model behaves?**\n",
        "**A:** No — observability is passive; it collects data, it doesn’t influence model output or logic.\n",
        "\n",
        "**Q: Can LangSmith trace external tools or APIs?**\n",
        "**A:** Yes — as long as those calls are part of your LangGraph run, they show up as runs.\n",
        "\n",
        "**Q: Is trace data kept forever?**\n",
        "**A:** By default, trace retention is bounded (e.g., 400 days in SaaS), but you can add traces to datasets for longer retention. ([LangChain Docs][5])\n",
        "\n",
        "**Q: Can I analyze prompt performance differences?**\n",
        "**A:** Yes — Playground and experiments let you compare model responses across inputs and configurations. ([LangChain Docs][6])\n",
        "\n",
        "---\n",
        "\n",
        "## What You See in the UI\n",
        "\n",
        "When you open LangSmith, you’ll typically interact with:\n",
        "\n",
        "* **Projects**: group of traces ([LangChain Docs][4])\n",
        "* **Traces & Runs**: detailed execution logs ([LangChain Docs][3])\n",
        "* **Threads**: chat-style grouped traces ([LangChain Docs][2])\n",
        "* **Playground**: prompt test & experiments ([LangChain Docs][6])\n",
        "* **Datasets & Evaluations**: structured test corpora ([LangChain Docs][7])\n",
        "\n",
        "These views let you debug, analyze cost, and iterate confidently.\n",
        "\n",
        "---\n",
        "\n"
      ],
      "metadata": {
        "id": "A6NVK-Mzt7Fa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "LangSmith works as a tracing and measurement layer. When a user messages your LangGraph chatbot, LangSmith captures the full turn as a **Trace**. Inside a trace you get **Runs**, representing each node, tool, or model used. Every run exposes input/output, latency, token usage, and execution-level details.\n",
        "\n",
        "This shifts your chatbot from opaque to inspectable: instead of guessing why a turn was slow or expensive, you can see precisely which node or LLM consumed time/tokens, how many retries occurred, what tools fired, and where failures originated.\n",
        "\n",
        "---\n",
        "\n",
        "## Core Integration Setup (Unchanged Behavior)\n",
        "\n",
        "Create an account at `smith.langchain.com` and generate an API key. Set the following environment variables:\n",
        "\n",
        "```python\n",
        "import os\n",
        "\n",
        "os.environ[\"LANGCHAIN_TRACING_V2\"] = \"true\"\n",
        "os.environ[\"LANGCHAIN_ENDPOINT\"] = \"https://api.smith.langchain.com\"\n",
        "os.environ[\"LANGCHAIN_API_KEY\"] = \"your_api_key_here\"\n",
        "os.environ[\"LANGCHAIN_PROJECT\"] = \"Chatbot_Project\"\n",
        "```\n",
        "\n",
        "This enables tracing without modifying application code paths.\n",
        "\n",
        "---\n",
        "\n",
        "## Trace & Run Semantics\n",
        "\n",
        "Definitions remain the same:\n",
        "\n",
        "● **Trace** = One user turn (input → output).\n",
        "\n",
        "● **Run** = Execution of a specific component inside a trace (LLM call, node, retriever, tool, etc).\n",
        "\n",
        "Within the UI you see:\n",
        "\n",
        "– raw message content\n",
        "– total token cost (input + output)\n",
        "– end-to-end latency\n",
        "– time-to-first-token\n",
        "– node-level execution tree\n",
        "\n",
        "This gives you a deterministic reconstruction of the request lifecycle.\n",
        "\n",
        "---\n",
        "\n",
        "## Thread-Based Organization for Multi-User Sessions\n",
        "\n",
        "Without metadata, all traces stream into a flat global list. For production chatbots this becomes chaotic. Thread IDs solve it.\n",
        "\n",
        "Clean invocation pattern:\n",
        "\n",
        "```python\n",
        "config = {\n",
        "    \"configurable\": {\n",
        "        \"thread_id\": st.session_state.thread_id\n",
        "    },\n",
        "    \"metadata\": {\n",
        "        \"thread_id\": st.session_state.thread_id\n",
        "    },\n",
        "    \"run_name\": \"Chat_Turn\"\n",
        "}\n",
        "\n",
        "events = chatbot.stream(\n",
        "    {\"messages\": [HumanMessage(content=user_input)]},\n",
        "    config=config,\n",
        "    stream_mode=\"values\"\n",
        ")\n",
        "```\n",
        "\n",
        "Result in UI: Threads render as distinct conversation timelines, similar to chat apps.\n",
        "\n",
        "---\n",
        "\n",
        "## Production Benefits (Clarified)\n",
        "\n",
        "1. **Visual Debugging**\n",
        "   Full request-level playback shows message evolution, latency, and token usage.\n",
        "\n",
        "2. **Threaded Conversations**\n",
        "   Enables chat history review for individual users or sessions, instead of global chaos.\n",
        "\n",
        "3. **Complex Pipeline Inspection**\n",
        "   Critical for architectures using:\n",
        "   – RAG with vector stores\n",
        "   – tool execution\n",
        "   – external API calls\n",
        "   – MCP (Model Context Protocol) or multi-plane control\n",
        "   – agent graphs with branching paths\n",
        "\n",
        "4. **LLMOps Tooling**\n",
        "   UI allows:\n",
        "   – model cost monitoring\n",
        "   – performance dashboards\n",
        "   – dataset export\n",
        "   – prompt testing playground\n",
        "   – regression testing\n",
        "\n",
        "All required for deployment at scale.\n",
        "\n",
        "---\n",
        "\n",
        "## Features Added for Clarity (Requested)\n",
        "\n",
        "Below are clean Q&A and feature-specific explanations without altering context.\n",
        "\n",
        "### Q&A Section\n",
        "\n",
        "**Q: Does LangSmith modify the model output?**\n",
        "A: No. It observes and records execution without intervention.\n",
        "\n",
        "**Q: Can I monitor token cost across a whole session?**\n",
        "A: Yes. Token usage aggregates per trace and per thread.\n",
        "\n",
        "**Q: Does this support non-LLM nodes?**\n",
        "A: Yes. Any node or tool call becomes a Run entry with its own timings.\n",
        "\n",
        "**Q: Can I debug RAG retrieval steps?**\n",
        "A: Yes. Runs reveal which documents were retrieved, ranked, and passed to the LLM.\n",
        "\n",
        "**Q: Is this required for LangGraph?**\n",
        "A: Not required, but critical for production where cost, performance, and correctness must be audited.\n",
        "\n",
        "---\n",
        "\n",
        "### Feature Breakdown (Condensed)\n",
        "\n",
        "✔ Execution tracing\n",
        "✔ Token accounting\n",
        "✔ Latency profiling\n",
        "✔ Thread organization\n",
        "✔ Node-level introspection\n",
        "✔ Dataset creation for evaluation\n",
        "✔ Prompt testing playground\n",
        "✔ Multi-Agent & RAG visibility\n",
        "✔ Observability dashboards\n",
        "✔ Cost monitoring\n",
        "\n",
        "---\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "7lvEEkujuHQo"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_E85WYiZuQce"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}